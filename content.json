{"pages":[],"posts":[{"title":"Java内存区域与内存溢出异常","text":"JDK主要是包括Java程序设计语言，Java SE API和Java虚拟机。Java内存管理是深入理解Java虚拟机的第一步。 Java虚拟机在执行java程序的过程中会把它管理的内存划分为若干个不同的数据区域。这些区域都有各自的用途，以及创建和销毁时间，有的区域随着虚拟机进程的启动而存在，有些区域则依赖用户线程的启动和结束而建立和销毁。Java虚拟机管理的内存包括如上图的几个运行时数据区域。 程序计数器程序计数器（Program Counter Register）是一块较小的内存区域，它可以看做是当前线程字节码的行号指示器。字节码解释器工作时就是通过改变这个计数器的值来选取下一条需要执行的字节码指令，分支、循环、跳转、异常处理、线程恢复等基础功能都要依赖这个计数器完成。由于Java虚拟机的多线程是通过线程轮流切换并分配处理器执行时间的方式来实现的，在任何时刻，一个处理器都只会执行一条线程中的指令，因此，为了线程切换后还能恢复到正确的执行位置，每条线程都需要一个独立的程序计数器，各个线程之间计数器互不影响，独立存储， 我们称这类存储区域为线程私有的内存。如果线程执行的是一个Java方法，这个计数器记录的是正在执行的虚拟机字节码指令的地址；如果线程执行的是一个本地方法，这个计数器的值则为空。 java虚拟机栈Java虚拟机栈（java Virtual Machine Stack）也是线程私有的，它的生命周期与线程相同。虚拟机栈描述的是Java方法执行的内存模型。每个方法执行的同时都会在虚拟机栈空间创建一个栈帧（Stack Frame）用于存储局部变量表、操作数栈、动态链接、方法出口等信息。其中局部变量表存储了编译期可知的各种基本数据类型（boolean、byte、char、short、int、float、long、double）、对象引用（reference类型，它不等同于对象本身，可能是一个指向对象起始地址的引用指针，也可能是一个代表对象的句柄或其他与此对象相关的位置）和returnAddress类型（指向了一条字节码指令的地址）。关于虚拟机栈可以参考虚拟机字节码执行引擎这一章 本地方法栈本地方法栈与java虚拟机栈类似，只是本地方法栈用于虚拟机调用Native方法。在Hotspot虚拟机中直接将本地方法栈和虚拟机栈合二为一了。 java堆Java堆是被所有线程共享的一块内存区域，在虚拟机启动时创建。Java堆存在的唯一目的是存放对象实例，几乎所有的对象实例都在这里分配。Java堆也是垃圾收集器管理的主要区域，因此很多时候也被称为”GC堆“。从内存回收的角度来看，由于现在收集器基本上都采用分代收集算法，所以Java堆还可以细分为：新生代和老年代；更细致的划分有Eden空间、From Survivor空间、To Survivor空间等。从内存分配的角度看，线程共享的Java堆中可能划分出多个线程私有的分配缓冲区（Threal Local Allocation Buffer, TLAB）。 Java堆可以处于物理上不连续的内存空间中，只要逻辑上是连续的即可。如果在对中没有内存完成实例分配，并且堆无法再扩展时，将会抛出OutOfMemoryError异常。 方法区方法区（Method Area）与Java堆一样，是各个线程共享的内存区域，它用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。Java虚拟机规范对方法区的限制非常宽松，除了和Java堆一样不需要连续的内存和可以选择固定大小或者可扩展外，还可以选择不实现垃圾收集。方法区的内存回收目标主要是针对常量池的回收和对类型的卸载。 运行时常量池运行时常量池（Runtime Constant Pool）是方法区的一部分。Class文件中除了有类的版本、字段、方法、接口等描述信息外，还有一项信息是常量池（Constant Pool Table），用于存放编译期生成的各种字面量和符号引用，这部分内容将在类加载后进入方法区的运行时常量池中存放。一般来说，运行时常量池除了保存Class文件中描述的符号引用外，还会把翻译出来的直接引用也存储在运行时常量池中。 运行时常量池相对于Class文件常量池的另外一个重要特性是具有动态性，在运行期间可能将新的常量放入池中。 直接内存直接内存（Direct Memory）既不是虚拟机运行时数据区的一部分，也不是Java虚拟机规范中定义的内存区域。在NIO中，可以使用Native函数库直接分配堆外内存，然后通过一个存储在java堆中的DirectByteBuffer对象作为这块内存的引用进行操作，避免了数据在Java堆和Native堆中来回地复制数据。 对象的创建虚拟机遇到一条new指令时，首先去检查这个指令的参数是否能在常量池中定位到一个类的符号引用，并且检查这个符号引用代表的类是否已被加载、解析和初始化过，如果没有，那必须先执行相应的类加载过程。关于类加载过程可以参考第7章。在类加载完成后，虚拟机将为对象分配内存，对象所需要内存的大小在类加载完成后就确定了。Java分配内存的方式跟垃圾回收紧密相关，如果垃圾回收会整理内存，java使用指针碰撞的方式分配内存，如果垃圾回收不会整理内存，java使用空闲列表的方式分配内存。 在Java中，为对象分配内存不是线程安全的，有两种方法保证线程安全：1，使用同步锁（低效），一种方式是采用CAS配上失败重试的方法保证更新的原子性（Java虚拟机普遍使用的方法，参考CAS&amp;volatile）；2，把内存分配的动作按照线程划分在不同的空间进行，即每个线程在Java堆中预先分配一小块内存，成为本地线程分配缓冲（TLAB）。那个下线程要分配内存，就在哪个线程的TLAB上分配，只有TLAB用完病分配新的TLAB时，才需要同步锁。内存分配完毕后，虚拟机将分配到的内存空间都初始化为零值，这也是新建对象时，属性都有默认值的原因。 接下来，虚拟机要对对象进行必要的设置，例如这个对象是哪个类的实例、如何才能找到对象的元数据信息、对象的哈希码、对象的GC分代年龄等信息，这些信息都存放在对象的对象头（Object Header）中。此时从虚拟机的角度来看，新对象已经生成了，但是从java语言的角度来看，对象的创建才刚刚开始，初始化方法都还没有执行。 对象的内存布局在Hotspot虚拟机中，对象内存中存储的信息分为3个区域：对象头（Header）、实例数据（Instance Data）和对齐填充（Padding）。 对象头分为两部分，第一部分用于存储对象自身的运行时数据，如哈希码、GC分代年龄、锁状态标志、线程持有的锁、偏向锁ID、偏向时间戳等，这部分数据成为”Mark Word“, 为32位（32位虚拟机）；第二部分为类型指针，指向它的类元数据的指针，虚拟机通过这个指针来确定这个对象是哪个类的实例。这个指针不是虚拟机所必须的。 Hotspot虚拟机的自动内存管理系统要求对象起始地址必须是8字节的整数倍，也就是说对象的大小必须是8字节的整数倍，不够的空间通过对齐填充补上。 对象的访问定位两种方法，1：直接访问，在虚拟机栈中存放对象的指针；2,：使用句柄，在java堆中单独划分一块内存作为句柄池，虚拟机栈存放句柄的指针，通过句柄间接访问。","link":"/2020/04/23/Java%E5%86%85%E5%AD%98%E5%8C%BA%E5%9F%9F%E4%B8%8E%E5%86%85%E5%AD%98%E6%BA%A2%E5%87%BA%E5%BC%82%E5%B8%B8/"},{"title":"SecurityContextHolder, SecurityContext 和 Authentication 对象","text":"SecurityContextHolder是应用存储安全相关详情的地方，默认情况下，SecurityContextHolder使用ThreadLocal进行存储，可以说它是线程安全的。 获取当前用户的数据SecurityContextHolder#getContext()用于获取当前用户的SecurityContext对象，如前所属，通过ThreadLocal能正确获取当前线程的SecurityContext. SecurityContext接口源码如下: 123456public interface SecurityContext extends Serializable { // 获取认证对象 Authentication getAuthentication(); // 设置认证对象 void setAuthentication(Authentication authentication);} 可以看出，这个接口主要用于表示认证对象的设置和获取功能，也就是说Spring Security使用Authentication对象表示用户认证信息。通常情况下我们可以通过以下代码获取用户的代理信息，并且这个代理信息在Spring Security中大部分情况下是UserDetails的实例，当然我们也可以自定义其实现。 1234567Object principal = SecurityContextHolder.getContext().getAuthentication().getPrincipal();if (principal instanceof UserDetails) { String username = ((UserDetails)principal).getUsername();} else { String username = principal.toString();} UserDetails源码 123456789101112131415161718192021222324252627282930313233343536public interface UserDetails extends Serializable { /** * 返回用户权限集合，不允许返回null */ Collection&lt;? extends GrantedAuthority&gt; getAuthorities(); /** * 返回用户密码 */ String getPassword(); /** * 返回用于认证的用户名，不允许null */ String getUsername(); /** * 账号是否过期，过期账号不允许认证 */ boolean isAccountNonExpired(); /** * 账号是否锁定，锁定的账号不允许认证 */ boolean isAccountNonLocked(); /** * 用户密码是否过去，过期密码不允许认证 */ boolean isCredentialsNonExpired(); /** * 用户是否解锁，未解锁用户禁止认证 */ boolean isEnabled();} 这个接口定义了大部分情况下需要的功能，可以通过继承这个接口进行扩展 Think of UserDetails as the adapter between your own user database and what Spring Security needs inside the SecurityContextHolder. – spring security doc UserDetailsService接口UserDetailsService接口只定义了一个方法: 1UserDetails loadUserByUsername(String username) throws UsernameNotFoundException; 在spring security中，最常用的加载用户信息的方法就是这个，UserDetailsService的不同实现一般对应了不同的用户信息加载机制，如InMemoryDaoImpl实现从内存中加载用户信息，JdbcDaoImpl则从数据库加载用户信息。可以通过自己实现这个接口自定义用户信息加载途径。 GrantedAuthority在Authentication中，方法getAuthorities()返回一个代表用户权限的集合，这种权限通常用”角色”表示，例如ROLE_ADMINSTRATOR或者ROLE_HR_SUPERVISOR. 小结 SecurityContextHolder提供获取SecurityContext的方法 SecurityContext持有Authentication实例和一些请求相关的安全信息 Authentication表示用户的认证信息 GrantedAuthority表示应用层面的用户的权限信息，通过角色表示 UserDetails提供构建Authentication的必要信息 UserDetailsService通过用户名参数创建UserDetails实例 AuthenticationSpring Security中的认证过程 用户被提示通过用户名和密码登录– 用户名和密码用于构建UsernamePasswordAuthenticationToken(这是Authentication接口的一个实现) 系统验证用户名正确– 将token作为AuthenticationManager的参数用于验证 用户的认证上下文信息被获取– AuthenticationManager返回一个完全填充的Authentication实例 为用户创建安全上下文– 通过SecurityContextHolder.getContext().setAuthentication(...)设置认证信息 用户基于当前上下文继续操作 Web环境中的认证典型web应用的认证过程 浏览主页，点击链接 请求到服务器，服务器发现客户端在访问受保护的资源 但是2中的用户并没有通过，于是服务器返回响应，要求客户端进行认证。这个响应既可以是一个Http状态码或重定向 依赖于认证以及，浏览器会重定向到指定的网页来让用户填写认证信息，或者浏览器通过其它方式来获取认证数据 浏览器向服务器返回一个HTTP响应，它可能是一个包含认证信息表单的请求，或者在HTTP头中设置了认证信息 浏览器决定认证数据是否有效，如果有效，就可以访问受限制的资源了；否则，继续要求认证 原先导致认证发生的请求会重试，如果用户权限足够，则请求成功；否则，返回”403” Spring Security对上述的每一步都会有特定的类进行响应。主要的类是ExceptionTranslationFilter, 核心逻辑如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public void doFilter(ServletRequest req, ServletResponse res, FilterChain chain) throws IOException, ServletException { HttpServletRequest request = (HttpServletRequest) req; HttpServletResponse response = (HttpServletResponse) res; try { // 这个过滤器没有任何自定义逻辑，只负责捕获异常链中的认证异常，对错误进行转换 chain.doFilter(request, response); logger.debug(\"Chain processed normally\"); } catch (IOException ex) { throw ex; } catch (Exception ex) { // Try to extract a SpringSecurityException from the stacktrace Throwable[] causeChain = throwableAnalyzer.determineCauseChain(ex); RuntimeException ase = (AuthenticationException) throwableAnalyzer .getFirstThrowableOfType(AuthenticationException.class, causeChain); if (ase == null) { ase = (AccessDeniedException) throwableAnalyzer.getFirstThrowableOfType( AccessDeniedException.class, causeChain); } if (ase != null) { if (response.isCommitted()) { throw new ServletException(\"Unable to handle the Spring Security Exception because the response is already committed.\", ex); } // 处理SpringSecurityException，核心逻辑 handleSpringSecurityException(request, response, chain, ase); } else { // Rethrow ServletExceptions and RuntimeExceptions as-is if (ex instanceof ServletException) { throw (ServletException) ex; } else if (ex instanceof RuntimeException) { throw (RuntimeException) ex; } // Wrap other Exceptions. This shouldn't actually happen // as we've already covered all the possibilities for doFilter throw new RuntimeException(ex); } } } 它提供的服务主要是捕获请求中的认证异常，并通过handleSpringSecurityException方法进行处理 AuthenticationEntryPointAuthenticationEntryPoint是一个认证入口，负责引导用户完成认证。Spring Security提供了几个默认实现： BasicAuthenticationEntryPoint：HTTP基本认证 DelegationAuthenticationEntryPoint: 维护了一个Map，可以根据不同的请求路径调用对应的认证入口 DigestAuthenticationEntryPoint: 摘要认证 Http403ForbiddenEntryPoint: 直接返回403 HttpStatusEntryPoint:返回指定的状态码 LoginUrlAuthenticationEntryPoint: 将请求转发到指定URL进行认证 在请求间保存SecurityContext在Spring Security中，SecurityContextPersistenceFilter过滤器负责保存SecurityContext，默认的实现侧率是将安全上下文作为HttpSession的一个属性进行存储. Spring Security中的访问控制AccessDecisionManager用于实现访问控制 AbstractSecurityInterceptor提供了一致性的工作流用于处理安全对象的请求，典型情况如下： 查询和当前请求相关的”配置属性” 提交安全对象，当前的Authentication以及配置属性给AccessDecisionManager用于权限判定 在调用发生的地方可以选择性地修改Authentication 如果权限正确，则允许调用 如果方法调用成功并且配置了AfterInvocationManager，则调用AfterInvocationManager; 如果调用抛出了异常，AfterInvocationManager就不会被调用 核心ServicesAuthenticationManager, ProviderManager和AuthenticationProviderProviderManager是AuthenticationManager接口的默认实现，但是它自己并不会处理认证相关的事情，而是委托给一个AuthenticationProvider List，其中的每一项都会检查自己是否能执行认证，每个provider要么抛出异常，要么返回一个完全填充的Authentication对象。 认证成功后擦除信息默认情况下，成功认证后ProviderManager会尝试擦除Authentication中的敏感信息，这是为了像密码等信息只在必要的时刻能访问。有时候这种做法会引起问题，比如在无状态的应用中，通过缓存用户数据来提供性能的时候。如果Authentication持有一个指向缓存的引用时，但是这个它的认证信息被擦除了，那样就无法通过认证来获取缓存了。可以考虑复制一个Authentication；另外的一种方法是在ProviderManager中禁止eraseCredentialsAfterAuthentication DaoAuthenticationProvider通过UserDetailsService在数据库中查询用户名，密码和GrantedAuthority. 认证过程也只是通过对UserDetailsService返回的UsernamePasswordAuthenticationToken来对比用户名和密码。 UserDetailsService实现 In-Memory JdbcDaoImpl 认证（Authentication）In-Memory AuthenticationJDBC AuthenticationLDAP AuthenticationAuthenticationProviderAuthenticationProvider Java Configuration","link":"/2020/04/24/spring/spring-security%E6%9E%B6%E6%9E%84%E5%92%8C%E5%AE%9E%E7%8E%B0/"},{"title":"Mysql系列之InnoDB存储引擎概述","text":"MySQL插件式的存储引擎架构提供了一系列标准的管理和服务支持，这些标准与存储引擎无关。存储引擎是基于表的，而不是数据库。对于开发人员来说，存储引擎对其是透明的，但了解各种存储引擎的区别对于开发人员来说也是有好处的。对于DBA来说，他们应该深刻地认识到MySQL数据库的核心在于存储引擎。 InnoDB存储引擎将数据放在一个逻辑的表空间中，这个表空间就像黑盒一样由InnoDB存储引擎自身进行管理。InnoDB将每个它管理的表单独存放到一个独立的.ibd文件中。此外，InnoDB存储引擎还支持用裸设备来建立其表空间。 InnoDB通过使用多版本并发控制（MVCC）来获得高并发性，并且实现了SQL标准的4种隔离级别，默认为REPEATABLE级别。同事，使用一种被称为next-key locking的策略来避免幻读（phantom）现象的产生。除此之外，InnoDB存储引擎还提供了插入缓冲（insert buffer）、二次写（double write）、自适应哈希索引（adaptive hash index）、预读（read ahead）等高性能和高可用性的功能。 对于表中数据的存储，InnoDB采用了聚集（clustered）的方式，因此每张表的存储都是按主键的顺序进行存放。如果没有显示地在表定义时指定主键，InnoDB会为每一行生成一个6字节的ROWID，并一次作为主键。","link":"/2020/04/24/mysql/mysql-innodb/"},{"title":"Leftmost Column with at Least a One","text":"(This problem is an interactive problem.) A binary matrix means that all elements are 0 or 1. For each individual row of the matrix, this row is sorted in non-decreasing order. Given a row-sorted binary matrix binaryMatrix, return leftmost column index(0-indexed) with at least a 1 in it. If such index doesn’t exist, return -1. You can’t access the Binary Matrix directly. You may only access the matrix using a BinaryMatrix interface: BinaryMatrix.get(row, col) returns the element of the matrix at index (row, col) (0-indexed). BinaryMatrix.dimensions() returns a list of 2 elements [rows, cols], which means the matrix is rows * cols. Submissions making more than 1000 calls to BinaryMatrix.get will be judged Wrong Answer. Also, any solutions that attempt to circumvent the judge will result in disqualification. For custom testing purposes you’re given the binary matrix mat as input in the following four examples. You will not have access the binary matrix directly. Example 1: 12Input: mat = [[0,0],[1,1]]Output: 0 Example 2: 12Input: mat = [[0,0],[0,1]]Output: 1 Example 3: 12Input: mat = [[0,0],[0,0]]Output: -1 Example 4: 12Input: mat = [[0,0,0,1],[0,0,1,1],[0,1,1,1]]Output: 1 Constraints: rows == mat.length cols == mat[i].length 1 &lt;= rows, cols &lt;= 100 mat[i][j] is either 0 or 1. mat[i] is sorted in a non-decreasing way. 思路因为每一行都是单调递增的，可以通过二叉搜索寻找1，考虑到有重复的1，在找到1的情况下，需要找到第一个。根据题意，需要找到第一个出现1的列，假设在第i行的第j列找到了一个1，那么在i+1行只需要在0~j-1列来寻找1了。 solution1234567891011121314151617181920212223242526272829303132333435363738394041/** * // This is the BinaryMatrix's API interface. * // You should not implement it, or speculate about its implementation * interface BinaryMatrix { * public int get(int row, int col) {} * public List&lt;Integer&gt; dimensions {} * }; */class Solution { public int leftMostColumnWithOne(BinaryMatrix binaryMatrix) { List&lt;Integer&gt; dimensions = binaryMatrix.dimensions(); int rows = dimensions.get(0); int cols = dimensions.get(1); int res = Integer.MAX_VALUE; for (int row = 0; row &lt; rows; row++) { int index = binarySearchOne(binaryMatrix, row, Math.min(res, cols-1)); res = Math.min(res, index); } return res == Integer.MAX_VALUE ? -1 : res; } private int binarySearchOne(BinaryMatrix binaryMatrix, int row, int cols) { int start = 0; int end = cols; while (start &lt;= end) { int mid = start + (end-start) / 2; int val = binaryMatrix.get(row, mid); if (val == 1) { while (mid &gt; 0 &amp;&amp; binaryMatrix.get(row, mid-1) == 1) { mid--; } return mid; } else { start = mid+1; } } return Integer.MAX_VALUE; }}","link":"/2020/04/26/leetcode/Leftmost-Column-with-at-Least-a-One/"},{"title":"LRU Cache","text":"Design and implement a data structure for Least Recently Used (LRU) cache. It should support the following operations: get and put. get(key) - Get the value (will always be positive) of the key if the key exists in the cache, otherwise return -1.put(key, value) - Set or insert the value if the key is not already present. When the cache reached its capacity, it should invalidate the least recently used item before inserting a new item. The cache is initialized with a positive capacity. Follow up:Could you do both operations in O(1) time complexity? Example: 1234567891011LRUCache cache = new LRUCache( 2 /* capacity */ );cache.put(1, 1);cache.put(2, 2);cache.get(1); // returns 1cache.put(3, 3); // evicts key 2cache.get(2); // returns -1 (not found)cache.put(4, 4); // evicts key 1cache.get(1); // returns -1 (not found)cache.get(3); // returns 3cache.get(4); // returns 4 思路根据LRU的性质可知，应该是需要一个链表来存储相关的数据的，这是因为删除将数据节点挪到前端的成本低（相较于数据），同时题目要求查找的时间也要是O(1)，因此还需要一个map对数据进行映射。为了在O(1)时间挪动数据节点，链表应该是双向链表。当从LRU列表中删除节点时，同时需要删除map中的key，这时需要有一个节点到map的key的映射，可以为此再新建一个map，但是在数据节点中直接存储这个key会更好一些。删除节点需要考虑头和尾的问题。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108class LRUCache { private Node head; private Node tail; private Map&lt;Integer, Node&gt; map; private int capacity; private int size; public LRUCache(int capacity) { this.capacity = capacity; map = new HashMap&lt;&gt;(); } public int get(int key) { Node node = map.get(key); if (node == null) { return -1; } if (node == head) { return node.val; } Node last = node.last; Node next = node.next; last.next = node.next; if (next != null) { next.last = last; } else { tail = last; } node.next = head; node.last = null; head.last = node; head = node; return head.val; } public void put(int key, int value) { Node targetNode = map.get(key); if (targetNode != null) { targetNode.val = value; if (targetNode != head) { Node next = targetNode.next; Node last = targetNode.last; last.next = next; if (next == null) { tail = last; } else { next.last = last; } targetNode.next = head; head.last = targetNode; head = targetNode; } return; } if (size == 0) { Node node = new Node(key, value); head = tail = node; size++; map.put(key, node); } else if (size &lt; capacity) { Node node = new Node(key, value); node.next = head; head.last = node; head = node; size++; map.put(key, node); } else { Node node = new Node(key, value); node.next = head; head.last = node; head = node; map.put(key, node); Node ex = tail; Node last = tail.last; ex.last = null; last.next = null; tail = last; map.remove(ex.key); } }}class Node { int key; int val; Node next; Node last; public Node(int key, int val) { this.key = key; this.val = val; }}/** * Your LRUCache object will be instantiated and called as such: * LRUCache obj = new LRUCache(capacity); * int param_1 = obj.get(key); * obj.put(key,value); */","link":"/2020/04/27/leetcode/LRU-Cache/"},{"title":"Maximal Square","text":"Given a 2D binary matrix filled with 0’s and 1’s, find the largest square containing only 1’s and return its area. Example: 12345678Input: 1 0 1 0 01 0 1 1 11 1 1 1 11 0 0 1 0Output: 4 思路这是一道dp题，定义dp[row][col]为第row行第col列的正方形的边长，显然，如果matrix[row][col] == 0，此时dp[row][col] = 0. 否则，根据dp[row-1][col-1]的值edge，从当前点往外延申，边长一直算到edge，如果这两边都为1，dp[row][col] = edge + 1. res = max(res, dp[row][col] ^2). 1234567891011121314151617181920212223242526272829303132333435363738394041class Solution { public int maximalSquare(char[][] matrix) { int res = 0; int height = matrix.length; if (height == 0) { return 0; } int width = matrix[0].length; if (width == 0) { return 0; } int[][] dp = new int[height][width]; for (int row = 0; row &lt; height; row++) { for (int col = 0; col &lt; width; col++) { if (matrix[row][col] == '0') { System.out.print(dp[row][col] + \" \"); continue; } if (row == 0 || col == 0 || dp[row-1][col-1] == 0) { dp[row][col] = 1; res = Math.max(res, 1); } else { int edge = 0; while (edge &lt; dp[row-1][col-1]) { if (matrix[row-edge-1][col] != '1' || matrix[row][col-edge-1] != '1') { break; } edge++; } dp[row][col] = 1 + edge; res = Math.max(res, dp[row][col] * dp[row][col]); } System.out.print(dp[row][col] + \" \"); } System.out.println(); } return res; }}","link":"/2020/04/27/leetcode/Maximal-Square/"},{"title":"Predict the Winner","text":"Given an array of scores that are non-negative integers. Player 1 picks one of the numbers from either end of the array followed by the player 2 and then player 1 and so on. Each time a player picks a number, that number will not be available for the next player. This continues until all the scores have been chosen. The player with the maximum score wins. Given an array of scores, predict whether player 1 is the winner. You can assume each player plays to maximize his score. Example 1: 123Input: [1, 5, 2]Output: FalseExplanation: Initially, player 1 can choose between 1 and 2. If he chooses 2 (or 1), then player 2 can choose from 1 (or 2) and 5. If player 2 chooses 5, then player 1 will be left with 1 (or 2). So, final score of player 1 is 1 + 2 = 3, and player 2 is 5. Hence, player 1 will never be the winner and you need to return False. Example 2: 123Input: [1, 5, 233, 7]Output: TrueExplanation: Player 1 first chooses 1. Then player 2 have to choose between 5 and 7. No matter which number player 2 choose, player 1 can choose 233.Finally, player 1 has more score (234) than player 2 (12), so you need to return True representing player1 can win. 思路这是一道博弈题，根据所学知识，博弈题可以根据相对得分来确定结果。玩家获胜的条件是在当前数据下，玩家获取的相对得分大于或等于0是赢得游戏的条件，按照游戏规则，当前玩家能获得的相对分数的公式应该是Math.max(nums[start]-score(nums, start+1, end), nums[end]-score(nums, start, end-1)).也就是玩家从头或者从尾部取值，减去另外一个玩家所能获取的最大分数，再在两个差值间寻找最大值即为本轮的相对分数。 12345678910111213141516class Solution { public boolean PredictTheWinner(int[] nums) { return score(nums, 0, nums.length-1) &gt;= 0; } private int score(int[] nums, int start, int end) { if (start &gt; end) { return 0; } if (start == end) { return nums[start]; } return Math.max(nums[start] - score(nums, start+1, end), nums[end] - score(nums, start, end-1)); }} 可以使用记忆化递归优化递归调用 123456789101112131415161718class Solution { private Integer[][] dp; public boolean PredictTheWinner(int[] nums) { dp = new Integer[nums.length][nums.length]; return score(nums, 0, nums.length-1) &gt;= 0; } private int score(int[] nums, int start, int end) { if (start &gt; end) { return 0; } if (dp[start][end] != null) { return dp[start][end]; } dp[start][end] = Math.max(nums[start] - score(nums, start+1, end), nums[end] - score(nums, start, end-1)); return dp[start][end]; }}","link":"/2020/04/28/leetcode/Predict-the-Winner/"},{"title":"First Unique Number","text":"You have a queue of integers, you need to retrieve the first unique integer in the queue. Implement the FirstUnique class: FirstUnique(int[] nums) Initializes the object with the numbers in the queue. int showFirstUnique() returns the value of the first unique integer of the queue, and returns -1 if there is no such integer. void add(int value) insert value to the queue. Example 1: 123456789101112131415Input: [&quot;FirstUnique&quot;,&quot;showFirstUnique&quot;,&quot;add&quot;,&quot;showFirstUnique&quot;,&quot;add&quot;,&quot;showFirstUnique&quot;,&quot;add&quot;,&quot;showFirstUnique&quot;][[[2,3,5]],[],[5],[],[2],[],[3],[]]Output: [null,2,null,2,null,3,null,-1]Explanation: FirstUnique firstUnique = new FirstUnique([2,3,5]);firstUnique.showFirstUnique(); // return 2firstUnique.add(5); // the queue is now [2,3,5,5]firstUnique.showFirstUnique(); // return 2firstUnique.add(2); // the queue is now [2,3,5,5,2]firstUnique.showFirstUnique(); // return 3firstUnique.add(3); // the queue is now [2,3,5,5,2,3]firstUnique.showFirstUnique(); // return -1 Example 2: 123456789101112131415Input: [&quot;FirstUnique&quot;,&quot;showFirstUnique&quot;,&quot;add&quot;,&quot;add&quot;,&quot;add&quot;,&quot;add&quot;,&quot;add&quot;,&quot;showFirstUnique&quot;][[[7,7,7,7,7,7]],[],[7],[3],[3],[7],[17],[]]Output: [null,-1,null,null,null,null,null,17]Explanation: FirstUnique firstUnique = new FirstUnique([7,7,7,7,7,7]);firstUnique.showFirstUnique(); // return -1firstUnique.add(7); // the queue is now [7,7,7,7,7,7,7]firstUnique.add(3); // the queue is now [7,7,7,7,7,7,7,3]firstUnique.add(3); // the queue is now [7,7,7,7,7,7,7,3,3]firstUnique.add(7); // the queue is now [7,7,7,7,7,7,7,3,3,7]firstUnique.add(17); // the queue is now [7,7,7,7,7,7,7,3,3,7,17]firstUnique.showFirstUnique(); // return 17 Example 3: 1234567891011Input: [&quot;FirstUnique&quot;,&quot;showFirstUnique&quot;,&quot;add&quot;,&quot;showFirstUnique&quot;][[[809]],[],[809],[]]Output: [null,809,null,-1]Explanation: FirstUnique firstUnique = new FirstUnique([809]);firstUnique.showFirstUnique(); // return 809firstUnique.add(809); // the queue is now [809,809]firstUnique.showFirstUnique(); // return -1 思路首先考虑使用数组存储唯一的数字，但是随着相同数据的加入，数据可能会移除数组。使用链表和HashMap的数据结构进行存储。链表只存储唯一的数字，map存储数字到节点的映射。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182class FirstUnique { Map&lt;Integer, Node&gt; map; Node head; Node tail; public FirstUnique(int[] nums) { map = new HashMap&lt;&gt;(); for (int num : nums) { add(num); } } public int showFirstUnique() { if (head == null) { return -1; } return head.num; } public void add(int value) { if (map.containsKey(value)) { removeNode(value); return; } Node node = new Node(value); addTail(node); map.put(value, node); } private void addTail(Node node) { if (head == null) { head = tail = node; } else { tail.next = node; node.last = tail; tail = node; } } private void removeNode(int val) { Node node = map.get(val); if (node == null) { return; } Node last = node.last; Node next = node.next; if (head == tail) { head = tail = null; } else if (last == null) { head = next; next.last = null; } else if (next == null) { last.next = null; tail = last; } else { last.next = next; next.last = last; node.next = null; node.last = null; } map.put(val, null); }}class Node { int num; Node last; Node next; Node(int num) { this.num = num; }}/** * Your FirstUnique object will be instantiated and called as such: * FirstUnique obj = new FirstUnique(nums); * int param_1 = obj.showFirstUnique(); * obj.add(value); */","link":"/2020/04/28/leetcode/First-Unique-Number/"},{"title":"Binary Tree Maximum Path Sum","text":"Given a non-empty binary tree, find the maximum path sum. For this problem, a path is defined as any sequence of nodes from some starting node to any node in the tree along the parent-child connections. The path must contain at least one node and does not need to go through the root. Example 1: 1234567Input: [1,2,3] 1 / \\ 2 3Output: 6 Example 2: 123456789Input: [-10,9,20,null,null,15,7] -10 / \\ 9 20 / \\ 15 7Output: 42 思路考虑节点n（必须包含节点n）能得到的最大和，它所能得到的最大值应该是它左节点能得到的最大值 + 右节点能得到的最大值和它自己的值，即maxScore(node) = maxScore(node.left) + maxScore(node.right) + node.val. 根据题目的意思，可以不从根节点开始算，那么就必须在递归求解子节点的最大分数时来更新最终的结果。这样，当求解根节点的maxScore时，我们得到的不一定是全局最大值，但是在递归求解的时候已经更新了全局最大值了。 123456789101112131415161718192021222324252627282930313233343536/** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode() {} * TreeNode(int val) { this.val = val; } * TreeNode(int val, TreeNode left, TreeNode right) { * this.val = val; * this.left = left; * this.right = right; * } * } */class Solution { int res; public int maxPathSum(TreeNode root) { res = Integer.MIN_VALUE; helper(root); return res; } private int helper(TreeNode root) { if (root == null) { return 0; } int left = Math.max(0, helper(root.left)); int right = Math.max(0, helper(root.right)); res = Math.max(res, left + right + root.val); return Math.max(left, right) + root.val; }}","link":"/2020/04/29/leetcode/Binary-Tree-Maximum-Path-Sum/"},{"title":"Range Sum of BST","text":"Given the root node of a binary search tree, return the sum of values of all nodes with value between L and R (inclusive). The binary search tree is guaranteed to have unique values. Example 1: 12Input: root = [10,5,15,3,7,null,18], L = 7, R = 15Output: 32 Example 2: 12Input: root = [10,5,15,3,7,13,18,1,null,6], L = 6, R = 10Output: 23 思路这是一道遍历二叉树的题目，可以通过前序，中序，后序实现。判断当前节点，如果节点的值在给定范围内，则将其加入到全局结果中。 前序遍历1234567891011121314151617181920212223242526272829303132/** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode() {} * TreeNode(int val) { this.val = val; } * TreeNode(int val, TreeNode left, TreeNode right) { * this.val = val; * this.left = left; * this.right = right; * } * } */class Solution { private int res; public int rangeSumBST(TreeNode root, int L, int R) { if (root == null) { return 0; } if (root.val &gt;= L &amp;&amp; root.val &lt;= R) { res += root.val; } rangeSumBST(root.left, L, R); rangeSumBST(root.right, L, R); return res; }} 中序遍历1234567891011121314151617181920212223242526272829303132/** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode() {} * TreeNode(int val) { this.val = val; } * TreeNode(int val, TreeNode left, TreeNode right) { * this.val = val; * this.left = left; * this.right = right; * } * } */class Solution { private int res; public int rangeSumBST(TreeNode root, int L, int R) { if (root == null) { return 0; } rangeSumBST(root.left, L, R); if (root.val &gt;= L &amp;&amp; root.val &lt;= R) { res += root.val; } rangeSumBST(root.right, L, R); return res; }} 后续遍历1234567891011121314151617181920212223242526272829303132/** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode() {} * TreeNode(int val) { this.val = val; } * TreeNode(int val, TreeNode left, TreeNode right) { * this.val = val; * this.left = left; * this.right = right; * } * } */class Solution { private int res; public int rangeSumBST(TreeNode root, int L, int R) { if (root == null) { return 0; } rangeSumBST(root.left, L, R); rangeSumBST(root.right, L, R); if (root.val &gt;= L &amp;&amp; root.val &lt;= R) { res += root.val; } return res; }}","link":"/2020/05/02/leetcode/Range-Sum-of-BST/"},{"title":"Merge Two Binary Trees","text":"Given two binary trees and imagine that when you put one of them to cover the other, some nodes of the two trees are overlapped while the others are not. You need to merge them into a new binary tree. The merge rule is that if two nodes overlap, then sum node values up as the new value of the merged node. Otherwise, the NOT null node will be used as the node of new tree. Example 1: 1234567891011121314Input: Tree 1 Tree 2 1 2 / \\ / \\ 3 2 1 3 / \\ \\ 5 4 7 Output: Merged tree: 3 / \\ 4 5 / \\ \\ 5 4 7 思路这道题的思路应该是递归实现的，先合并根节点，再合并左子树，最后合并右子树。可能出现的情况是当前节点有一个为null，那当前节点的取值为不为null的值，而子树则为不为null节点的子树。 123456789101112131415161718192021222324252627282930313233/** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode() {} * TreeNode(int val) { this.val = val; } * TreeNode(int val, TreeNode left, TreeNode right) { * this.val = val; * this.left = left; * this.right = right; * } * } */class Solution { public TreeNode mergeTrees(TreeNode t1, TreeNode t2) { TreeNode newNode; if (t1 == null &amp;&amp; t2 == null) { return null; } else if (t1 == null) { newNode = new TreeNode(t2.val); } else if (t2 == null) { newNode = new TreeNode(t1.val); } else { newNode = new TreeNode(t1.val + t2.val); } newNode.left = mergeTrees(t1 == null ? null : t1.left, t2 == null ? null : t2.left); newNode.right = mergeTrees(t1 == null ? null : t1.right, t2 == null ? null : t2.right); return newNode; }} 上述解法对于节点为null的情况没有进行优化，实际上，当碰到一个节点为null时，后续的递归调用没有必要再进行了，只需要将另一个节点的子树作为子树复制给当前节点即可。 123456789101112131415161718192021222324252627282930313233343536373839404142/** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode() {} * TreeNode(int val) { this.val = val; } * TreeNode(int val, TreeNode left, TreeNode right) { * this.val = val; * this.left = left; * this.right = right; * } * } */class Solution { public TreeNode mergeTrees(TreeNode t1, TreeNode t2) { TreeNode newNode; if (t1 == null &amp;&amp; t2 == null) { return null; } else if (t1 == null) { newNode = new TreeNode(t2.val); } else if (t2 == null) { newNode = new TreeNode(t1.val); } else { newNode = new TreeNode(t1.val + t2.val); } if (t1 == null) { // 碰到null节点可以停止递归调用了 newNode.left = t2 == null ? null : t2.left; newNode.right = t2 == null ? null : t2.right; } else { // 也是跳过null节点的递归调用 newNode.left = t2 == null ? t1.left : mergeTrees(t1.left, t2.left); newNode.right = t2 == null ? t1.right : mergeTrees(t1.right, t2.right); } // newNode.left = mergeTrees(t1 == null ? null : t1.left, t2 == null ? null : t2.left); // newNode.right = mergeTrees(t1 == null ? null : t1.right, t2 == null ? null : t2.right); return newNode; }}","link":"/2020/05/02/leetcode/Merge-Two-Binary-Trees/"},{"title":"N-ary Tree Preorder Traversal","text":"Given an n-ary tree, return the preorder traversal of its nodes’ values. Nary-Tree input serialization is represented in their level order traversal, each group of children is separated by the null value (See examples). Follow up: Recursive solution is trivial, could you do it iteratively? Example 1: 12Input: root = [1,null,3,2,4,null,5,6]Output: [1,3,5,6,2,4] Example 2: 12Input: root = [1,null,2,3,4,5,null,null,6,7,null,8,null,9,10,null,null,11,null,12,null,13,null,null,14]Output: [1,2,3,6,7,11,14,4,8,12,5,9,13,10] 思路不管多少叉树，前序遍历的顺序是先根节点，再子节点；后续遍历的顺序是先子节点，再根节点。对于多叉树，中序遍历没什么意义，因为根节点不知道放在子节点的什么位置，对于二叉树，中序遍历是左子树，根节点，右子树。 递归调用12345678910111213141516171819202122232425262728293031323334353637383940/*// Definition for a Node.class Node { public int val; public List&lt;Node&gt; children; public Node() {} public Node(int _val) { val = _val; } public Node(int _val, List&lt;Node&gt; _children) { val = _val; children = _children; }};*/class Solution { public List&lt;Integer&gt; preorder(Node root) { List&lt;Integer&gt; res = new ArrayList&lt;&gt;(); preorder(root, res); return res; } private void preorder(Node root, List&lt;Integer&gt; list) { if (root == null) { return; } list.add(root.val); List&lt;Node&gt; children = root.children; for (Node child : children) { if (child != null) { preorder(child, list); } } }} follow up使用迭代实现与二叉树的迭代遍历一样，都可以使用栈这种数据结构来完成，需要记住的是子节点的压栈顺序应该是从右往左的，这样才能保证出栈的顺序是从左往右的，这也是栈这种数据结构的性质决定的。在java语言种，Deque这种数据结构也叫双端队列，虽然也有Stack这种数据结构实现了栈，但是Stack是一个同步类，因为Stack继承Vector，这本身就是一个很蠢的设计，然而Deque又违背了封装的原则，相关的吐槽可以参考这里http://baddotrobot.com/blog/2013/01/10/stack-vs-deque/。我倾向于使用Deque来实现Stack。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546/*// Definition for a Node.class Node { public int val; public List&lt;Node&gt; children; public Node() {}n public Node(int _val) { val = _val; } public Node(int _val, List&lt;Node&gt; _children) { val = _val; children = _children; }};*/class Solution { public List&lt;Integer&gt; preorder(Node root) { if (root == null) { return new ArrayList&lt;&gt;(); } Deque&lt;Node&gt; deque = new ArrayDeque&lt;&gt;(); List&lt;Integer&gt; res = new ArrayList&lt;&gt;(); deque.offerLast(root); while (!deque.isEmpty()) { Node node = deque.pollLast(); res.add(node.val); List&lt;Node&gt; children = node.children; if (children != null &amp;&amp; !children.isEmpty()) { int size = children.size(); for (int i = size-1; i &gt;= 0; i--) { Node curr = children.get(i); if (curr != null) { deque.offerLast(curr); } } } } return res; } } 但目前的问题是为什么使用栈的执行时间比递归调用还长？！！！","link":"/2020/05/04/leetcode/N-ary-Tree-Preorder-Traversal/"},{"title":"Push Dominoes","text":"There are N dominoes in a line, and we place each domino vertically upright. In the beginning, we simultaneously push some of the dominoes either to the left or to the right. After each second, each domino that is falling to the left pushes the adjacent domino on the left. Similarly, the dominoes falling to the right push their adjacent dominoes standing on the right. When a vertical domino has dominoes falling on it from both sides, it stays still due to the balance of the forces. For the purposes of this question, we will consider that a falling domino expends no additional force to a falling or already fallen domino. Given a string “S” representing the initial state. S[i] = 'L', if the i-th domino has been pushed to the left; S[i] = 'R', if the i-th domino has been pushed to the right; S[i] = '.', if the i-th domino has not been pushed. Return a string representing the final state. Example 1: 12Input: &quot;.L.R...LR..L..&quot;Output: &quot;LL.RR.LLRRLL..&quot; Example 2: 123Input: &quot;RR.L&quot;Output: &quot;RR.L&quot;Explanation: The first domino expends no additional force on the second domino. Note: 0 &lt;= N &lt;= 10^5 String dominoes contains only 'L‘, 'R' and '.' 思路如果从左右两边来看的话，在左边，往左边push的牌会把所有它左边的牌往左推倒，直到碰到往右推的牌。同理，在右边，往右边push的牌会把所有它右边的牌往右推倒，直到碰到往左推的牌。还有一种情况是，以左边为例，往左倒的牌在碰到它能碰到的第一个往右倒的牌之前已经有往左倒的牌了，这部分的牌应该全部往左倒。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960class Solution { public String pushDominoes(String dominoes) { int N = dominoes.length(); char[] res = new char[N]; int lastRightPush = -1; int lastLeftPush = -1; for (int i = 0; i &lt; N; i++) { char c = dominoes.charAt(i); if (c == '.') { res[i] = c; } else if (c == 'L') { res[i] = c; if (lastRightPush &gt; lastLeftPush) { // R...L for (int j = lastRightPush+1; j &lt; i; j++) { int leftLen = j - lastRightPush; int rightLen = i - j; if (leftLen &lt; rightLen) { res[j] = 'R'; } else if (leftLen == rightLen) { res[j] = '.'; } else { res[j] = 'L'; } } } else if (lastRightPush &lt; lastLeftPush) { // L...L for (int j = lastLeftPush+1; j &lt;i; j++) { res[j] = 'L'; } } else { // ...L for (int j = 0; j &lt; i; j++) { res[j] = c; } } lastLeftPush = i; } else { res[i] = 'R'; if (lastLeftPush &lt; lastRightPush) { // R...R for (int j = lastRightPush+1; j &lt; i; j++) { res[j] = 'R'; } } lastRightPush = i; } } if (lastRightPush &gt; lastLeftPush) { // R... for (int i = lastRightPush; i &lt; N; i++) { res[i] = 'R'; } } return String.valueOf(res); }} 参考方法一，只记录符号为’L’或者’R’的符号以及位置，对每个区间有以下三种情形: “L…..L” 或者”R……R”, 此时只需要更新中间的状态为边上的即可, “R…..L”, 计算中间牌的状态，并更新 “L……R” 中间牌的状态不变 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152class Solution { public String pushDominoes(String dominoes) { int N = dominoes.length(); int[] indexes = new int[N+2]; char[] symbols = new char[N+2]; indexes[0] = -1; symbols[0] = 'L'; int l = 1; for (int i = 0; i &lt; N; i++) { char c = dominoes.charAt(i); if (c != '.') { indexes[l] = i; symbols[l] = c; l++; } } indexes[l] = N; symbols[l] = 'R'; l++; char[] res = dominoes.toCharArray(); for (int index = 0; index &lt; l-1; index++) { int start = indexes[index]; int end = indexes[index+1]; char x = symbols[index]; char y = symbols[index+1]; if (x == y) { // L.....L or R.....R for (int i = start+1; i &lt; end; i++) { res[i] = x; } } else if (x &gt; y) { // R....L for (int i = start+1; i &lt; end; i++) { int leftLen = i - start; int rightLen = end - i; if (leftLen == rightLen) { res[i] = '.'; } else if (leftLen &lt; rightLen) { res[i] = 'R'; } else { res[i] = 'L'; } } } } return String.valueOf(res); }} 方法二，从左往右计算往右推的力，从右计算往左推的力，最后根据力的状态来决定当前位置牌的状态。 123456789101112131415161718192021222324252627282930313233343536373839404142434445class Solution { public String pushDominoes(String dominoes) { int N = dominoes.length(); int[] forces = new int[N]; int force = 0; for (int i = 0; i &lt; N; i++) { char c = dominoes.charAt(i); if (c == 'R') { force = N; } else if (c == 'L') { force = 0; } else { force = Math.max(force - 1, 0); } forces[i] += force; } // cal force diff for (int i = N-1; i &gt;= 0; i--) { char c = dominoes.charAt(i); if (c == 'L') { force = N; } else if (c == 'R') { force = 0; } else { force = Math.max(force - 1, 0); } forces[i] -= force; } char[] res = new char[N]; for (int i = 0; i &lt; N; i++) { if (forces[i] == 0) { res[i] = '.'; } else if (forces[i] &gt; 0) { res[i] = 'R'; } else { res[i] = 'L'; } } return String.valueOf(res); }}","link":"/2020/05/03/leetcode/Push-Dominoes/"},{"title":"HashMap基础","text":"hashmap的实现hashmap也叫散列表，是一种使用算术操作将键转化为数组的索引来访问数组中的键值对的数据结构。使用hashmap进行查找分为两步。第一步：使用散列函数（hash function）将被查找的键转化为数组中的一个索引；第二步：处理碰撞冲突。 hash 函数因为每种数据类型都需要相应的散列类型，Java令所有数据类型都继承了一个能够返回一个32bits整数的hashCode()方法。每一种数据类型的hashCode()方法都必须和equals()方法一致。也就是说，如果a.equals(b)返回true，那么a.hashCode()的返回值必然和b.hashCode()的返回值相同。相反，如果两个对象的hashCode()不同，那么这两个对象的方法a.equals(b)也不相同。即使，a.hashCode() == b.hashCode(), a.equals(b)也是有可能返回false的。因此，要判断两个对象是否相等，必须同时判断hashCode()返回的值相等，equlas()方法返回true才行。默认情况下，hashCode()方法返回的是对象的内存地址，但是Java为很多常用的数据类型重写了hashCode()方法（包括String、Integer、Double、File和URL）。 为什么要处理碰撞冲突因为现实世界中，键的取值空间可能是无穷大的，我们用来存储键的数组却是有限大的。或者当键的取值空间大于我们用来存储数据的存储空间时，就有可能使得不同的键hash函数计算后映射到同一个数组的索引上。这时候就需要处理碰撞冲突。 碰撞冲突用于解决hash函数碰撞冲突的方法通常有两种：拉链法和线性探测法。 基于拉链法的HashMap拉链法的实现逻辑是将数组中的每个元素指向一条链表，链表中的每个结点都存储了hash值为该元素的索引的键值对。这个方法的基本思想就是选择足够大的M（数组大小），是的所有链表都尽可能短以保证高效的查找。 基于线性探测法的散列表使用大小为M的数组保存N个键值对，其中M&gt;N. 需要依靠数组中的空位来解决碰撞冲突。当碰撞发生时（当一个键的hash值已经被另一个不同的键占用，我们直接检查散列表中的下一个位置），这样的线性探测可能会产生三种结果： 命中，该位置的键和被查找的键相同； 未命中，键为空； 继续查找，该位置的键和被查找的键不同。 线性探测法的平均时间成本取决于元素在插入数组后聚集成的一组连续的条目，也叫键簇。显然短小的键簇才能保证较高的效率，而随着元素的插入，长键簇出现的可能性也越来越大。对于删除操作，我们不仅要将数组中的值置空，还要将键簇右侧的所有键重新插入到散列表。这可能是一个比较耗时的操作 扩容和缩容对于拉链法实现的hashmap，理论上是可以不需要扩容的。但是，考虑到查找的效率（链表不宜过长），当插入的元素的数量达到一定标准时，需要对数据进行扩容。假定当前的元素数量为N，数组大小为M，则hashmap的使用率为N/M, 当N/M大于某个值时，则需要对hashmap进行扩容。扩容的方法时先申请一个更大的数组，然后将所有的键重新散列并插入到新表中。同理，当删除元素达到一定数量时，为了防止空间浪费，需要对hashmap进行缩容。而缩容时对应的使用率应该比扩容时低，这是因为扩容和缩容都是很耗时的操作，如果在使用率为a时对hashmap扩容，在使用率低于a时进行缩容，则当使用率==a时，执行一次插入操作，map就得扩容，如果接下来的操作是一次删除操作，则又需要进行缩容。可以结合施密特触发器这种延时电路的原理来理解（来自电路专业的理解。。。）。","link":"/2020/05/04/data%20structure%20&%20algorithms/HashMap%E5%9F%BA%E7%A1%80/"},{"title":"HashMap in Java 7","text":"Java 7的HashMap是基于数组和单向链表实现的。如图所示 一些比较重要的字段 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566/** * 默认初始容量 - MUST be a power of two. */static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; // aka 16/** * 最大容量 * MUST be a power of two &lt;= 1&lt;&lt;30. */static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30;/** * 负载因子 */static final float DEFAULT_LOAD_FACTOR = 0.75f;/** * 空表对应的数组 */static final Entry&lt;?,?&gt;[] EMPTY_TABLE = {};/** * 实际使用的表. Length MUST Always be a power of two. */transient Entry&lt;K,V&gt;[] table = (Entry&lt;K,V&gt;[]) EMPTY_TABLE;/** * map中键值对的数量 */transient int size;/** * 下次扩容时，表中键值对的数量 * @serial */// If table == EMPTY_TABLE then this is the initial capacity at which the// table will be created when inflated.int threshold;/** * 负载因子 * * @serial */final float loadFactor;/** * The number of times this HashMap has been structurally modified * Structural modifications are those that change the number of mappings in * the HashMap or otherwise modify its internal structure (e.g., * rehash). This field is used to make iterators on Collection-views of * the HashMap fail-fast. (See ConcurrentModificationException). */transient int modCount;/** * The default threshold of map capacity above which alternative hashing is * used for String keys. Alternative hashing reduces the incidence of * collisions due to weak hash code calculation for String keys. * &lt;p/&gt; * This value may be overridden by defining the system property * {@code jdk.map.althashing.threshold}. A property value of {@code 1} * forces alternative hashing to be used at all times whereas * {@code -1} value ensures that alternative hashing is never used. */static final int ALTERNATIVE_HASHING_THRESHOLD_DEFAULT = Integer.MAX_VALUE; 构造器123456789101112131415161718192021222324252627282930313233343536373839/** * 通过指定的大小和负载因子构造 * capacity and load factor. * * @param initialCapacity the initial capacity * @param loadFactor the load factor * @throws IllegalArgumentException if the initial capacity is negative * or the load factor is nonpositive */public HashMap(int initialCapacity, float loadFactor) { if (initialCapacity &lt; 0) throw new IllegalArgumentException(\"Illegal initial capacity: \" + initialCapacity); if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException(\"Illegal load factor: \" + loadFactor); this.loadFactor = loadFactor; threshold = initialCapacity; init();}/** * 使用给定容量和默认负载因子构造 * capacity and the default load factor (0.75). */public HashMap(int initialCapacity) { this(initialCapacity, DEFAULT_LOAD_FACTOR);}/** * 使用默认容量和负载因子参数构造 * (16) and the default load factor (0.75). */public HashMap() { this(DEFAULT_INITIAL_CAPACITY, DEFAULT_LOAD_FACTOR);} 可以看到实际的构造函数是HashMap(int initialCapacity, float loadFactor)，它先对参数进行检查，如初始容量不能为负值，也不能超过最大容量，负载因子不能为空，不能小于或等于0，然后对实例变量进行赋值，最后调用init()进行初始化。 实际上init()方法什么都没干，真正对table变量分配空间的动作发生在put操作。以下是init()的实现逻辑: 123456789/** * Initialization hook for subclasses. This method is called * in all constructors and pseudo-constructors (clone, readObject) * after HashMap has been initialized but before any entries have * been inserted. (In the absence of this method, readObject would * require explicit knowledge of subclasses.) */ void init() { } 通过注释可看出，这个方法是子类的初始化钩子。也就是说，当HashMap被实例化时，这个方法会被调用，还没此时没有任何元素被插入时，HashMap是没有做任何操作，但是存在有些子类会在这里分配存储空间的可能。如LinkedHashMap就覆写了这个方法: 1234void init() { header = new Entry&lt;&gt;(-1, null, null, null); header.before = header.after = header;} LinkedHashMap的基本存储是链表，在这里声明了一个哑头节点。关于LinkedHashMap本文不作深究。 插入操作从构造函数可以发现，在构造函数中，是没有实现空间分配的。HashMap的插入操作通过方法put(K key, V val)实现的，源码如下： 12345678910111213141516171819202122public V put(K key, V value) { if (table == EMPTY_TABLE) { inflateTable(threshold); } if (key == null) return putForNullKey(value); int hash = hash(key); int i = indexFor(hash, table.length); for (Entry&lt;K,V&gt; e = table[i]; e != null; e = e.next) { Object k; if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) { V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; } } modCount++; addEntry(hash, key, value, i); return null;} 插入元素分为如下几步: 如果数组为空，分配存储空间 如果插入的键为null，单独调用putForNullKey(value)方法处理key为null的情形 对于正常的key，通过方法hash(key)计算key的hash值 通过3中的hash值得到key在数组中的索引i 遍历索引为i中的链表，如果找到了相同的key，则更新这个key的值。 否则通过addEntry(hash, key, value, i);方法增加一个新的节点。 以上的每一步都值得研究研究。 分配存储空间对于一个刚刚实例化的hashmap，此时table肯定是只想EMPTY_TABLE的，因此需要扩容，具体逻辑如下： 123456789private void inflateTable(int toSize) { // Find a power of 2 &gt;= toSize int capacity = roundUpToPowerOf2(toSize); threshold = (int) Math.min(capacity * loadFactor, MAXIMUM_CAPACITY + 1); // 分配存储空间 table = new Entry[capacity]; initHashSeedAsNeeded(capacity);} 可以看到，扩容都是向上取值到2的整数次幂，为什么？？？ 123456private static int roundUpToPowerOf2(int number) { // assert number &gt;= 0 : \"number must be non-negative\"; return number &gt;= MAXIMUM_CAPACITY ? MAXIMUM_CAPACITY : (number &gt; 1) ? Integer.highestOneBit((number - 1) &lt;&lt; 1) : 1;} 重点是Integer.highestOneBit((number - 1) &lt;&lt; 1)，假定传入的值已经是2的整数次幂了，将它减1再向左移1位，然后取最高位，得到的结果与原数相同，如number = 8, 对应的二进制值为1000, 减1为0111，再向左移1位为1110，取最高一位为1000。对于不为1的值，如14，二进制值为1110, 减1为1101，再左移1位位11010，取最高位为10000。这样保证了返回值必然是2的整数次幂。（感觉可以作为一道leetcode题了）。 接着是计算下次扩容的阈值，然后分配存储空间，最后是初始化hashSeed，参考Q&amp;A. 处理key为null的情形putForNullKey(value)的实现逻辑如下： 12345678910111213private V putForNullKey(V value) { for (Entry&lt;K,V&gt; e = table[0]; e != null; e = e.next) { if (e.key == null) { V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; } } modCount++; addEntry(0, null, value, 0); return null;} 这个操作跟put方法类似，只是在for循环里判断的是链表中的元素的key是否为null，如果是则更新。可以看出，null 键实际上是存储在第一个slot里面的。而且，从这个for循环可看出，如果是更新值，put方法返回的是旧值，如果是插入操作，put返回的是null。这个可以总结为，put方法返回的是当前key的上一个值（不存在时为null）。 计算key的hash值hash(Object key)方法的实现如下： 123456789101112131415final int hash(Object k) { int h = hashSeed; if (0 != h &amp;&amp; k instanceof String) { // 这是一个补丁。。。 return sun.misc.Hashing.stringHash32((String) k); } h ^= k.hashCode(); // This function ensures that hashCodes that differ only by // constant multiples at each bit position have a bounded // number of collisions (approximately 8 at default load factor). h ^= (h &gt;&gt;&gt; 20) ^ (h &gt;&gt;&gt; 12); return h ^ (h &gt;&gt;&gt; 7) ^ (h &gt;&gt;&gt; 4);} 忽略对String类型进行的hash方法，这涉及到一个tomcat提交的bug。为什么？？？ 做这么多移位异或的目的是为了避免碰撞。 索引计算indexFor(hash, table.length);实现如下： 1234static int indexFor(int h, int length) { // assert Integer.bitCount(length) == 1 : \"length must be a non-zero power of 2\"; return h &amp; (length-1);} 通过pre-hash的结果和数组的长度按位与即可。这是因为length-1必定是一个”00001111“形式的值（因为length必须是2的n次方），这种做法是通过h有意义的位数来决定在数组中的位置。 查询与更新可以发现，对于非null值的key，判断插入的key已经存在于表中的条件是：e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))。意思是hash值必须相等，这样才能是相同的索引，key必须相等，如果是基本数据类型，==即可，如果是对象，==只能判断是否指向同一个实例，在插入新的key时，这个条件一般为false，此时就必须通过equals方法来进行判断，关于java中的equals方法，可以参考这里！！！ 插入新的节点addEntry(hash, key, value, i);的实现逻辑如下： 12345678910111213141516/** * Adds a new entry with the specified key, value and hash code to * the specified bucket. It is the responsibility of this * method to resize the table if appropriate. * * Subclass overrides this to alter the behavior of put method. */void addEntry(int hash, K key, V value, int bucketIndex) { if ((size &gt;= threshold) &amp;&amp; (null != table[bucketIndex])) { resize(2 * table.length); hash = (null != key) ? hash(key) : 0; bucketIndex = indexFor(hash, table.length); } createEntry(hash, key, value, bucketIndex);} 可以发现，在插入新的值的时候，会判断是否需要扩容，扩容的条件是：当前hashmap的大小已经达到阈值了，并且给定的索引处已有entry了。可以发现，即使hashmap已经达到容量限制了，但是仍存在插入新的entry而不会发生扩容的可能。 扩容的实现逻辑如下： 123456789101112131415161718192021222324252627/** * Rehashes the contents of this map into a new array with a * larger capacity. This method is called automatically when the * number of keys in this map reaches its threshold. * * If current capacity is MAXIMUM_CAPACITY, this method does not * resize the map, but sets threshold to Integer.MAX_VALUE. * This has the effect of preventing future calls. * * @param newCapacity the new capacity, MUST be a power of two; * must be greater than current capacity unless current * capacity is MAXIMUM_CAPACITY (in which case value * is irrelevant). */void resize(int newCapacity) { Entry[] oldTable = table; int oldCapacity = oldTable.length; if (oldCapacity == MAXIMUM_CAPACITY) { threshold = Integer.MAX_VALUE; return; } Entry[] newTable = new Entry[newCapacity]; transfer(newTable, initHashSeedAsNeeded(newCapacity)); table = newTable; threshold = (int)Math.min(newCapacity * loadFactor, MAXIMUM_CAPACITY + 1);} 扩容也是hashmap中非常重要的一个操作，可以看到它的实现逻辑大致是先判断一下已有数据的大小是否已达上限，如果达到了，则将阈值置为上线即可，因为已经无法扩容了。然后分配新的空间，再通过transfer(Entry[] newTable, boolean rehash)方法将数据转移到新的数组中，将table指向新的数组，重transfer(Entry[] newTable, boolean rehash)新计算下一个阈值。 具体实现如下： 123456789101112131415161718/** * Transfers all entries from current table to newTable. */void transfer(Entry[] newTable, boolean rehash) { int newCapacity = newTable.length; for (Entry&lt;K,V&gt; e : table) { while(null != e) { Entry&lt;K,V&gt; next = e.next; if (rehash) { e.hash = null == e.key ? 0 : hash(e.key); } int i = indexFor(e.hash, newCapacity); e.next = newTable[i]; newTable[i] = e; e = next; } }} 这个过程就是遍历老的数据，然后重新计算hash和索引，进行一边插入操作。为什么要rehash？？？ 至此hashmap的插入操作也就差不多了。 删除操作删除操作的源码如下： 12345678910111213/** * Removes the mapping for the specified key from this map if present. * * @param key key whose mapping is to be removed from the map * @return the previous value associated with &lt;tt&gt;key&lt;/tt&gt;, or * &lt;tt&gt;null&lt;/tt&gt; if there was no mapping for &lt;tt&gt;key&lt;/tt&gt;. * (A &lt;tt&gt;null&lt;/tt&gt; return can also indicate that the map * previously associated &lt;tt&gt;null&lt;/tt&gt; with &lt;tt&gt;key&lt;/tt&gt;.) */public V remove(Object key) { Entry&lt;K,V&gt; e = removeEntryForKey(key); return (e == null ? null : e.value);} 可以看到，删除操作会返回key对应的val。removeEntryForKey(key)的实现如下： 12345678910111213141516171819202122232425262728293031323334/** * Removes and returns the entry associated with the specified key * in the HashMap. Returns null if the HashMap contains no mapping * for this key. */final Entry&lt;K,V&gt; removeEntryForKey(Object key) { if (size == 0) { return null; } int hash = (key == null) ? 0 : hash(key); int i = indexFor(hash, table.length); Entry&lt;K,V&gt; prev = table[i]; Entry&lt;K,V&gt; e = prev; while (e != null) { Entry&lt;K,V&gt; next = e.next; Object k; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) { modCount++; size--; if (prev == e) table[i] = next; else prev.next = next; e.recordRemoval(this); return e; } prev = e; e = next; } return e;} Q &amp; A为什么capacity必须是2的n次方因为hashmap通过hash值求数组的index是通过h &amp; (length-1)实现的，length是2的n次方保证了length-1的值对应的位数都为1，通过按位与操作时间上是一个求模的运算，只是速度更快。如果不是2的n次方，则length-1会出现n位以下的某一些位数为0，导致有的index永远不会用到。 为什么hash函数要有那么复杂的移位与异或操作首先异或操作可以理解为不进位的加操作，某一位的结果与操作数在该位上的取值都相关。观察h &amp; (length-1)，可以发现，假设length-1的结果中为1的位数为x, 则按位求与的结果实际上只与h的低x位相关，如果只是使用系统固有的hashCode或者被复写的很差的hashCode()函数，可能导致大量的碰撞。做这么多的移位操作实际上是为了将高位的值的影响叠加到低位，使得hashCode()的所有位数都参与到计算中，避免碰撞。 并发修改死锁的问题并发修改导致的死锁问题其实不算是一个问题，因为作者有明确说明HashMap不是线程安全的，如果非要在多线程环境下使用，会出现并发修改死锁的问题。参考这里。 为什么会有initHashSeedAsNeeded(int capacity)这个需要参考CVE-2011-4858。是说，在Java中，String的hashCode()设计的不够好，如下： 123456789101112public int hashCode() { int h = hash; if (h == 0 &amp;&amp; value.length &gt; 0) { char val[] = value; for (int i = 0; i &lt; value.length; i++) { h = 31 * h + val[i]; } hash = h; } return h;} 这个hash函数的实现大致就是遍历每一位字符，将上一个结果乘31再与当前值相加。这个设计存在这大量碰撞的可能。可以通过设置系统属性值，使得当阈值大于给定值时，使用替代的hash方法来对String进行hash计算。 views","link":"/2020/05/03/leetcode/HashMap-in-Java7/"},{"title":"ch03-文件","text":"MySQL数据库和InnoDB存储引擎中的文件如下： 参数文件：告诉MySQL实例启动时在哪里可以找到数据库文件，并且指定某些初始化参数，这些参数定义了某种内存结构的大小等设置。 日志文件：用来记录MySQL实例对某种条件做出响应时写入的文件，如错误日志文件、二进制日志文件、慢查询日志文件、查询日志文件等。 socket文件：当用UNIX域套接字方式进行连接时需要的文件。 pid文件：MySQL实例的进程ID文件。 MySQL表结构文件：用来存放MySQL表结构定义的文件。 存储引擎文件：因为MySQL表存储引擎的关系，每个存储引擎都会有自己的文件来保存各种数据。这些存储引擎真正存储了记录和索引等数据。 参数文件参数文件用来配置系统并指定一些初始化参数，MySQL数据库的参数分为两类： 动态（dynamic）参数。MySQL实例运行时可更改。 静态（static）参数。MySQL实例运行时不可更改，read-only. 日志文件日志文件记录了影响MySQL数据库的各种类型活动。MySQL数据库常见的日志文件如下： 错误日志（error log） 二进制日志（binlog） 慢查询日志（slow query log） 查询日志（log） 错误日志错误日志文件对MySQL的启动、运行、关闭过程进行了记录。遇到问题应该首先查看该文件以便定位问题。错误日志通常能帮助用户定位问题。 慢查询日志慢查询日志（slow query）可帮助用户定位可能存在问题的SQL语句，从而进行SQL语句层面的优化。MySQL默认情况下没有开启慢查询日志，需要用户手动开启。参数log_queries_not_using_indexes用于记录没有使用索引的SQL到慢查询日志中。 查询日志查询日志记录了所有对MySQL数据库请求的信息，无论这些请求是否得到了正确的执行。 二进制日志二进制日志（binary log）记录了对MySQL数据库执行更改的所有操作，但是不包括SELECT和SHOW这类操作。即使操作本身并没有导致数据库发生变化，该操作还是可能会写入二进制日志。二进制日志的作用主要有以下几点： 恢复（recovery）：某些数据的恢复需要二进制日志，例如，在一个数据库全备文件恢复后，用户可以通过二进制日志进行point-in-time恢复。 复制（replication）：其原理与恢复类似，通过复制和执行二进制日志使一台远程的MySQL数据库（一般称为slave或standby）与一台MySQL数据库（一般称为master或primary）进行实时同步。 审计（audit）：用户可以通过二进制日志中的信息来进行审计，判断是否有对数据库进行注入攻击等。 二进制日志文件在默认情况下并没有启动，需要手动指定参数来启动。开启这个选项对数据库的性能影响很小，但是带来的收益很大。 当使用事务的表存储引擎时，所有未提交（uncommitted）的二进制会被记录到一个缓存中，等该事务提交（committed）时直接将缓冲中的二进制日志写入二进制日志文件。binlog_cache_size是基于会话的，也就是说，当一个线程开始一个事务时，MySQL会自动分配一个大小为binlog_cache_size的缓存。 在默认情况下，二进制日志并不是每次写的时候同步到磁盘。因此，当数据库所在操作系统发生宕机时，可能会有最后一部分数据没有写入二进制日志文件中，这会给恢复和复制带来问题，参数sync_binlog=[N]表示每写缓冲多少次就同步到磁盘。如果N=1，表示采用同步写磁盘的方式来写二进制日志，这时写操作不使用操作系统的缓冲来写二进制日志。sync_binlog的默认值为0，如果使用InnoDB存储引擎进行复制，并且想得到最大的高可用性，建议将该值设为ON，此时可能对数据库的IO造成一定的影响。 当使用InnoDB存储引擎时，即使sync_binlog=1, 在一个事务COMMIT动作之前，会先将二进制日志写到磁盘，如果这时已经写入了二进制日志，但是提交还没发生，并且此时宕机了，那么在MySQL数据库下次启动时，由于COMMIT操作并没有发生，这个事务会被回滚掉。但是二进制日志已经记录了该事务信息，不能被回滚。这个问题可以通过将参数innodb_support_xa设为1来解决。虽然这个参数与XA事务有关，但它同时也确保了二进制日志和InnoDB存储引擎数据文件的同步。 binlog_format参数影响了二进制日志的格式，该参数可设的值为STATEMENT、ROW和MIXED。 STATEMENT格式和老版本的一样，二进制日志记录的是日志的逻辑SQL语句 在ROW格式下，二进制日志记录的不再是简单的SQL语句了，而是记录表的行更改情况。在该格式下，可以将InnoDB的事务隔离级别设置为READ COMMITTED 在MIXED格式下，MySQL默认采用STATEMENT格式进行二进制日志文件的记录，但是在一些情况下使用ROW格式： 表的存储引擎为NDB，这时对表的DML操作都以ROW格式记录； 使用了UUID()、USER()、CURRENT_USER()、FOUND_ROWS()、ROW_COUNT()等不确定函数 使用了INSERT DELAY语句 使用了用户定义函数(UDF) 使用了临时表（temporary table） 套接字文件在UNIX系统下本地连接MySQL可以采用UNIX域套接字方式，这种方式需要一个套接字（socket）文件。 pid文件当MySQL实例启动时，会将自己的进程ID写入一个文件中——该文件就是pid文件。 表结构定义文件因为MySQL插件式存储引擎的体系结构的关系，MySQL数据的存储是根据表进行的，每个表都会有与之对应的文件。但不论采用何种存储引擎，MySQL都由一个以frm为后缀的文件，这个文件记录了该表的表结构定义。除此之外，frm还用来存放视图的定义。 InnoDB存储引擎文件表空间文件InnoDB采用将存储的数据按表空间（tablespace）进行存放的设计。在默认配置下会有一个初始大小为10MB，名为ibdata1的文件，该文件就是默认的表空间文件。所有基于InnoDB存储引擎的表的数据都会记录到该共享表空间中。 重做日志文件在默认情况下，在InnoDB存储引擎的数据目录下会有两个名为ib_logfile0和ib_logfile1的文件，这就是重做日志文件。重做日志文件记录了对于InnoDB存储引擎的事务日志。当实例或介质失败时，重做日志能用于将存储引擎的状态恢复至掉电前的状态，以此来保证数据的完整性。每个InnoDB存储引擎至少有1个重做日志文件组（group），每个文件组下至少有2个重做日志文件。在日志组中每个重做日志文件的大小一致，并以循环写入的方式运行。 二进制日志与重做日志的区别： 二进制日志会记录所有与MySQL数据库相关的日志记录；InnoDB存储引擎的重做日志只记录有关该存储引擎本身的日志； 二进制日志记录的是关于一个事务的具体操作内容，该日志是逻辑日志；InnoDB存储引擎的重做日志记录的是关于每个页（Page）的更改的物理状况； 二进制文件仅在事务提交前进行提交，即只写磁盘一次，不论这个事务有多大；在事务进行的过程中，不断有重做日志（redo entry）被写入到重做日志中； 可以看到重做日志条目由4个部分组成： redo_log_type：占用一个字节，表示重做日志类型； space表示表空间的id，但采用压缩方式，因此占用的空间可能小于4字节； page_no表示页的偏移量，同样采用压缩方式； redo_log_body：表示每个重做日志的数据部分，恢复时需要调用相应的函数进行解析。 重做日志的写入过程如下： 可以看到重做日志使先写到重做日志缓冲，然后按照一定的顺序写入到日志文件的，按512字节（1个扇区）。因为扇区是写入的最小单位，可以保证写入必定是成功的。因此在重做日志的写入过程中不需要double write. 参数innodb_flush_log_at_trx_commit的有效值为0、1、2. 0代表当提交事务时，并不将事务的重做日志写入磁盘上的日志文件，而是等待主线程每秒的刷新；1表示在执行commit时，将重做日志缓冲同步写入到磁盘，即伴有fsync的调用；2表示将重做日志异步写到磁盘，即写到文件系统的缓存中。因此不能完全保证在执行commit时肯定会写入重做日志文件。 为了保证事务的ACID中的D，必须将innodb_flush_log_at_trx_commit设置为1，也就是每当有事务提交时，就必须确保事务都已经写入重做日志文件了。","link":"/2020/05/04/mysql/ch03-%E6%96%87%E4%BB%B6/"},{"title":"Number Complement","text":"Given a positive integer, output its complement number. The complement strategy is to flip the bits of its binary representation. Example 1: 123Input: 5Output: 2Explanation: The binary representation of 5 is 101 (no leading zero bits), and its complement is 010. So you need to output 2. Example 2: 123Input: 1Output: 0Explanation: The binary representation of 1 is 1 (no leading zero bits), and its complement is 0. So you need to output 0. Note: The given integer is guaranteed to fit within the range of a 32-bit signed integer. You could assume no leading zero bit in the integer’s binary representation. This question is the same as 1009: https://leetcode.com/problems/complement-of-base-10-integer/ 思路求补的第一想法是与1进行异或，因为 1 ^ 1 = 0, 0 ^ 1 = 1. 所以只需要找到有效位全部为1的数与给定的值进行异或运算即可。java的Integer提供了一个方法highestOneBit()用于求给定数字的最高位为1的数，拿到这个数，左移一位，然后减1，则右边的位数都为1. 12345class Solution { public int findComplement(int num) { return ((Integer.highestOneBit(num) &lt;&lt; 1) - 1) ^ num; }} 思路二：将当前数字不停左移，直到最左边的1到达第32位，记录下移位的次数，然后将这个数按位取反，再向左移位同样的次数即可。如000000…. 101, 左移后变成101000….000, 按位取反为010111….111, 右移后为000…010. 12345678910class Solution { public int findComplement(int num) { int shifts = 0; while (num &gt; 0) { shifts++; num &lt;&lt;= 1; } return (~num) &gt;&gt;&gt; shifts; }} 思考：java中的数据是怎么表示的，移位操作实际上是怎么实现的？？","link":"/2020/05/04/leetcode/Number-Complement/"},{"title":"Isomorphic Strings","text":"Given two strings s\\ and t\\, determine if they are isomorphic. Two strings are isomorphic if the characters in s\\ can be replaced to get t\\. All occurrences of a character must be replaced with another character while preserving the order of characters. No two characters may map to the same character but a character may map to itself. Example 1: 12Input: s = &quot;egg&quot;, t = &quot;add&quot;Output: true Example 2: 12Input: s = &quot;foo&quot;, t = &quot;bar&quot;Output: false Example 3: 12Input: s = &quot;paper&quot;, t = &quot;title&quot;Output: true Note:You may assume both s\\ and t\\ have the same length. 思路一开始考虑使用一个map存储不同字母之间的映射，比如第一个例子”egg”和”add”, e-&gt;a; g-&gt;d. 提交发现是有问题的，因为对于”bar”和”foo”这种情况上述方法失效，a和r都会映射到o，不满足题意。最后使用两个map实现字母之间的映射，此时，第一个o对应a，但是到第二个o时，发现它对应的时r，应该返回false 12345678910111213141516171819202122232425262728293031323334class Solution { public boolean isIsomorphic(String s, String t) { int len1 = s.length(); int len2 = t.length(); if (len1 != len2) { return false; } Map&lt;Character, Character&gt; from = new HashMap&lt;&gt;(); Map&lt;Character, Character&gt; to = new HashMap&lt;&gt;(); for (int i = 0; i &lt; len1; i++) { char c1 = s.charAt(i); char c2 = t.charAt(i); Character mc1 = from.get(c1); Character mc2 = to.get(c2); if (mc1 != null &amp;&amp; mc1 != c2) { return false; } if (mc2 != null &amp;&amp; mc2 != c1) { return false; } if (mc1 == null) { from.put(c1, c2); } if (mc2 == null) { to.put(c2, c1); } } return true; }}","link":"/2020/05/04/leetcode/Isomorphic-Strings/"},{"title":"ch04-表","text":"简单来说，表就是关于特定实体的数据集合，这也是关系型数据库模型的核心。 索引组织表在InnoDB存储引擎中，表都是根据主键顺序组织存放的，这种存储方式的表称为索引组织表（index organized table）. 如果在创建表时没有显式地定义主键，则InnoDB会按照以下方式选择或创建主键： 首先判断表中是否有非空的唯一索引（Unique NOT NULL），如果有，则该列即为主键 如果上述条件不符，InnoDB存储引擎或自动创建一个6字节大小的指针作为主键 主键的选择根据的是定义索引的顺序，而不是建表时列的顺序 _rowid只能用于查看单个列为主键的情况，对于多列组成的主键就显得无能为力了。 InnoDB逻辑存储结构所有数据都被逻辑地存放在一个空间中，称为表空间（tablespace）。表空间又由段（segment）、区（extent）、页（page）组成。页有时候也称为块（block）。 表空间默认情况下，InnoDB有一个共享表空间ibdata1，即所有数据都存放在这个表空间内。如果用户启用了参数innodb_file_per_table，则每张表的数据可以单独放到一个表空间内，但此时，每张表的表空间内存放的只是数据、索引和插入缓冲Bitmap页，其他类的数据，如回滚（undo）信息、插入缓冲索引页、系统事务信息、二次写缓冲等还是存放在原来的共享表空间中。 段表空间是由各个段组成的，常见的段有数据段、索引段、回滚段等。因为InnoDB存储引擎中的表是索引组织的，所以数据即索引，索引即数据。在上图中，Leaf node segment即为数据段，Non-Leaf node segment即为索引段 区区是由连续页组成的空间，在任何情况下每个区的大小都为1MB。在默认情况下，InnoDB页的大小为16KB，即一个区中一共有64个连续的页。 在用户启动参数innodb_file_per_table后，创建表的默认大小是96KB，不足1MB，这是因为在每个段开始时，先用32个页大小的碎片页（fragment page）来存放数据，在使用完这些页之后才是64个连续页的申请。这样做的目的是，对于一些小表，或者是undo这类的段，可以在开始时申请较少的空间，节省磁盘容量的开销。 页页时InnoDB磁盘管理的最小单位，在InnoDB中，默认每个页的大小为16KB，可以通过参数innodb_page_size将页的大小设置为4K、8K、16K，若设置完成，则所有表中页的大小都为innodb_page_size，不可对其再次进行修改，除非通过mysqldump导入和导出操作来产生新的库。在InnoDB中，常见的页有以下几种： 数据页（B-Tree Node） undo页（undo Log Page） 系统页（System Page） 事务数据页（Transaction system ） 插入缓冲位图页（Insert Buffer Bitmap） 插入缓冲空闲列表页（Insert Buffer Free List） 未压缩的二进制大对象页（Uncompressed BLOB Page） 压缩的二进制大对象页（Compressed BLOB Page） 行InnoDB存储引擎是面向行的（row-oriented），也就是说数据是按行进行存放的。每个页存放的行记录也是有硬性规定的，最多允许存放16KB/2-200行的记录，即7992行记录。 InnoDB行记录格式InnoDB存储引擎提供了Compact和Redundant两种格式来存放行记录数据，默认格式是Compact格式，Redundant格式是为兼容老版本而保留的。 Compact行记录格式一个页中存放的行数据越多，其性能就越高。Compact行记录的存储方式如下： Compact格式行记录的首部是一个非NULL的变长字段长度列表，并且是按照列的顺序逆序放置的。其长度为： 若列的长度小于255字节，用1字节表示； 若列的长度大于255字节，用2字节表示 变长字段的长度最大不可以超过2字节，这是因为MYSQL数据库中VARCHAR类型的最大长度限制为65535. NULL标志位指示了该行数据中是否有NULL值，有用1表示，没有用0表示，这个字段的长度会随着NULL字段的增加而以字节为单位增加，比如当NULL字段数超过为9时，NULL标志位将至少占用两个字节。 记录头信息固定为5个字节，每位含义如下： 最后的部分就是实际存储每个列的数据。NULL不占用该部分任何空间，即NULL除了占用NULL标志位，实际存储不占用任何空间；每行数据除了用户定义的列外，还有两个隐藏列，事务ID和回滚指针，分别为6字节和7字节的大小，若InnoDB表没有定义主键，每行还会增加一个6字节的rowid列。 next_record记录的是页中下一条记录的相对位置，即当前记录的位置加上偏移量就是下一条记录的起始位置。所以InnoDB存储引擎在页内部是通过一种链表的结构来串联各个行记录的。 Redundant行记录格式 老格式，回头整理 行溢出数据InnoDB存储引擎可以将一条记录中的某些数据存储在真正的数据页之外。一般认为BLOB、LOB这类的大对象列类型的存储会把数据存放在数据页面之外，实际上BLOB可以不将数据放在溢出页面，即使是VARCHAR列数据类型，依然有可能被存放为行溢出数据。 InnoDB存储引擎并不支持65535长度的VARCHAR，这是因为还有别的开销。首先VARCHAR（N）中N指的是字符的长度，而VARCHAR类型最大支持65535字节，考虑到编解码的问题，65535个字符对应的字节数一般是大于65535的。 MySQL官方手册中定义的65535长度是指所有VARCHAR列的长度总和，如果列的长度总和超过这个长度，建表时就会报错。 由于InnoDB页的大小最大为16KB，即16384个字节，因此，在一般情况下，InnoDB存储引擎的数据都是存放在页类型为B-tree node中，但是当发生行溢出时，数据存放在页类型为Uncompress BLOB页中。对于行溢出的数据存储如下： InnoDB存储引擎表是索引组织的，即B+Tree的结构，这样每个页中至少应该有两条行记录（否则失去了B+Tree的意义，变成链表了）。因此，如果页中只能存放下一条记录，那么InnoDB存储引擎会自动将行数据存放到溢出页中。这条规则适用于VARCHAR、BLOB等类型的列，当数据存放在BLOB Page时，数据页只保存数据的前768字节。 Compressed和Dynamic行记录格式在新的InnoDB版本中引入了新的文件格式（Barracuda文件格式），支持两种新的行记录格式：Compressed和Dynamic. 新的两种记录格式对于存放在BLOB页中的数据采用了完全的行溢出方式，如图所示： 这种行记录格式在数据页只存放20字节的指针，实际的数据都存放在Off Page中，Compressed行记录格式的另一个功能就是，存储在其中的行数据会以zlib的算法进行压缩，因此对于BLOB、TEXT、VARCHAR这类大长度类型的数据能够进行非常有效的压缩存储。 CHAR的行结构存储虽然CHAR是存储固定长度的字符类型，但是对于固定长度的字符，某些字符编码会将相同长度的字符编码成不同长度的字节。因此，对于多字节字符编码的CHAR数据类型的存储，InnoDB存储引擎在内部将其视为变长字符类型。这也就意味着在变长长度列表中会记录CHAR数据类型的长度。 可以认为，在多字节字符集的情况下，CHAR和VARCHAR的实际行存储基本上是没有区别的。 InnoDB数据页结构InnoDB存储引擎管理数据库的最小磁盘单位是页，页类型为B-tree Node的页存放的即是表中行的实际数据了。InnoDB数据页由以下7个部分组成，具体如图所示： File Header（文件头） Page Header（页头） Infimun和Supremum Records User Records（用户记录，即行） Free Space（空闲空间） Page Directory （页目录） File Trailer （文件结尾信息） File HeaderFile Header用来记录页的一些头信息，用来记录页的版本，checksum，页的类型，页的偏移量等信息 Page HeaderPage Header用来记录数据页的状态信息。如堆中的记录数，该页中记录的数量，修改当前页的最大事务ID等 Infimum和Supremum Record在InnoDB存储引擎中，每个数据页有两个虚拟的行记录，用来限定记录的边界。Infimum记录是比该页中任何主键值都要小的值，Supremum指比任何可能大的值还要大的值。这两个值在页创建时被建立，并且在任何情况下不会被删除。 User Records和Free SpaceUser Record就是实际存储行记录的内容，InnoDB存储引擎总是B+树索引组织的。Free Space指的就是空闲空间，同样也是个链表数据结构。在一条记录被删除后，该空间会被加入到空闲链表中。 Page DirectoryPage Directory(页目录)中存放了记录的相对位置（注意，这里存放的是也相对位置，而不是偏移量），有些时候这些记录指针称为Slots（槽）或目录槽（Directory Slots）。在InnoDB中，并不是每个记录都拥有一个槽，InnoDB存储引擎的槽是一个稀疏目录（sparse directory），即一个槽中可能包含多个记录。伪记录Infimum的n_owned值总是1，记录Supremum的n_owned的取值范围是[1,8]，其他用户记录n_owned的取值范围为[4,8].当记录被插入或删除时需要对槽进行分裂或平衡的维护操作。 在Slots中记录按照索引键值顺序存放，这样可以利用二分查找迅速找到记录的指针。由于在InnoDB中Page Directory是稀疏目录，二叉查找树的结果只是一个粗略的结果，因此InnoDB存储引擎必须通过record header中的next_record来继续查找相关记录。 B+索引本身并不能找到具体的一条记录，能找到只是该记录所在的页。数据库把页载入到内存，然后通过Page Directory再进行二分查找，只不过二分查找的时间复杂度很低，同时在内存中查找很快，因此通常可以忽略这部分的查询时间。 File Trailer为了检测页是否已经完整地写入磁盘，InnoDB存储引擎的页中设置了File Tralier部分。File Trailer只有一个FIL_PAGE_END_LSN部分，占用8字节。前4字节代表该页的checksum值，后4字节和File Header中的FIL_PAGE_LSN相同。将这两个值与File Header中的FIL_PAGE_SPACE_OR_CHKSUM和FIL_PAGE_LSN值进行比较，看是否一致，以此来保证页的 完整性。 在默认配置下，InnoDB存储引擎每次从磁盘读取一个页就会检测该页的完整性，即页是否发生Corrupt，这就是通过File Trailer部分进行检测的，而该部分的检测会有一定的开销。 Named File Formats机制随着新的页数据结构不断出现并用于支持新的功能特性，为了解决不同版本下页结构兼容性的问题，InnoDB引入了Named File Formats机制。新的文件格式总是包含之前版本的页格式。参数innodb_file_format用来指定文件格式. 约束数据完整性关系型数据库本身能保证存储数据的完整性，不需要应用程序的控制。一般来说，数据完整性有以下三种形式： 实体完整性保证表中有一个主键。在InnoDB存储引擎表中，用户可以通过定义Primary Key或Unique Key约束来保证实体的完整性。用户还可以通过编写一个触发器来保证数据完整性。 域完整性保证数据每列的值满足特定的条件。在InnoDB中，域完整性可以通过以下几种途径来保证： 选择合适的数据类型确保一个数据值满足特定的条件 外键（Foreign Key）约束 编写触发器 考虑用DEFAULT约束作为强制域完整性的一个方面 参照完整性保证两张表之间的关系。InnoDB存储引擎支持外键，因此允许用户定义外键以强制参照完整性，也可以通过编写触发器以强制执行。 对InnoDB而言，本身提供了一下几种约束： Primary Key Unique Key Foreign Key Default NOT NULL 约束的创建和查找约束的创建可以采用以下两种方式： 表建立时就进行约束定义 利用ALTER TABLE命令来进行创建约束 约束和索引的区别约束是一个逻辑概念，用来保证数据的完整性；索引是一个数据结构，既有逻辑上的概念，在数据库中还代表着物理存储的方式。 对错误数据的约束在某些默认的情况下，MySQL数据库允许非法的或不正确的数据的插入或更新，又或者可以在数据库内部将其转化为一个合法的值，如向NOT NULL的字段插入一个NULL值，MySQL数据库会将其更改为0再进行插入。数据库本身没有对数据的正确性进行约束。通过设置参数sql_mode的值为STRICT_TRANS_TABLES，MySQL数据库对于输入值的合法性进行了约束，而且针对不同的错误，提示的错误内容也不相同。 ENUM和SET约束MySQL数据库不支持传统的CHECK约束，但是通过ENUM和SET类型可以解决部分离散值的约束，对于传统CHECK约束支持的连续值的范围约束或更复杂的约束，用户需要通过触发器来实现对于值域的约束。 触发器与约束触发器的作用是在执行INSERT、DELETE和UPDATE命令之前或之后自动调用SQL命令或存储过程。创建触发器的命令是CREATE TRIGGER，只有具备Super权限的用户才能执行这条命令，一张表最多能创建6个触发器，即分别为INSERT、UPDATE、DELETE的BEFORE和AFTER各定义一个。 通过触发器，用户可以实现MySQL数据库本身并不支持的一些特性，如对于传统CHECK约束的支持，物化视图，高级复制，审计等特性。 外键约束外键用来保证参照完整性，MySQL数据库的MyISAM存储引擎本身并不支持外键，对于外键的定义只是起到一个注释作用。而InnoDB存储引擎则完整支持外键约束。 一般来说，称被引用的表为父表，引用的表为子表。可定义的子表操作有： CASCADE. 当父表发生DELETE或UPDATE操作时，对相应的子表中的数据也进行DELETE或UPDATE操作。 SET NULL. 当父表发生DELETE或UPDATE操作时，相应的子表中的数据被更新为NULL值，但是子表中相应的列必须允许为NULL值。 NO ACTION. 当父表发生DELETE或UPDATE操作时，抛出错误，不允许这类操作发生。 RESTRICT. 当父表发生DELETE或UPDATE操作时，抛出错误，不允许这类操作发生。 对于参照完整性约束，外键能起到一个非常好的作用。但是对于数据的导入操作，外键往往导致在外键约束的检查上花费大量时间。因为MySQL数据库是即时检查的，所以对导入的每一行都会进行外键检查。 视图在MySQL数据库中，视图（View）是一个命名的虚表，它由一个SQL查询来定义，可以当做表使用。与持久表（permanent table）不同的是，视图中的数据没有实际的物理存储。 视图的作用视图的主要用途之一是被用做一个抽象装置，特别是对于一些应用程序，程序本身不需要关心基表（base table）的结构，只需要按照视图定义来取数据或更新数据。虽然视图是基于基表的一个虚拟表，但是用户可以对某些视图进行更新操作，其本质就是通过视图的定义来更新基本表。一般称可以进行更新操作的视图为可更新视图（updatable view）。 物化视图MySQL不支持 分区表分区功能并不是在存储引擎层面完成的，MySQL数据库支持的分区类型为水平分区，并不支持垂直分区。此外，MySQL数据库的分区是局部分区索引，一个分区中即存放了数据又存放了索引，而全局分区是指，数据存放在各个分区中，但是所有数据的索引存放在一个对象中。 水平分区：将同一张表中不同的行记录分配到不同的物理文件中。 垂直分区：将同一张表中不同列的记录分配到不同的物理文件中。 分区可能会给某些SQL语句性能带来提高，但是分区主要用于数据库高可用性的管理。当前MySQL数据库支持以下几种类型的分区： RANGE分区：行数据基于属于一个给定连续区间的列值被放入分区； LIST分区：和RANGE分区类似，只是LIST分区面向的是离散的值； HASH分区：根据用户自定义的表达式的返回值来进行分区，返回值不能为负； KEY分区：根据MySQL数据库提供的哈希函数进行分区。 不论创建何种类型的分区，如果表中存在主键或唯一索引时，分区列必须是唯一索引的一个组成部分。唯一索引可以允许是NULL值得，并且分区列只要是唯一索引的一个组成部分，不需要整个唯一索引都是分区列。如果建表时没有指定主键和唯一索引，可以指定任何一个列为分区列。 分区类型RANGE类型启动分区之后，表不再是由一个ibd文件组成了，而是由建立分区时的各个分区idb文件组成。 RANGE分区主要用于日期列的分区，例如对于销售类的表，可以根据年来分区存放销售记录，这样创建的好处是便于对销售表的管理。如果我们要删除2008年的数据，只需要删除2008年数据所在的分区即可；另一个好处是可以加快某些查询操作，如果我们只需要查询2008年整年的销售额，SQL优化器会只查询2008对应的分区，而不会去搜索所有的分区，这称为Partition Pruning （分区修剪） LIST分区LIST分区和RANGE分区非常相似，只是分区列的值是离散的，而非连续的。 HASH分区HASH分区的目的是将数据均匀地分布到预先定义的各个分区中，保证各分区的数据数量大致都是一样的。要使用HASH分区来分割一个表，要在CREATE TABLE语句上添加一个”PARTITION BY HASH(expr)”子句，其中”expr“是一个返回一个整数的表达式。 KEY分区KEY分区和HASH分区相似，不同之处在于HASH分区使用用户定义的函数进行分区，KEY分区使用MySQL数据库提供的函数进行分区。 COLUMNS分区COLUMNS分区可视为RANGE分区和LIST分区的一种进化版本。COLUMNS分区可以直接使用非整型的数据进行分区，分区根据类型直接比较而得，不需要转化为整型。此外，RANGECOLUMNS分区可以对多个列的值进行分区。 COLUMNS分区支持的数据类型 所有整型类型，如INT、SMALLINT、TINYINT、BIGINT。不支持FOLAT和DECIMAL类型 日期类型，如DATE和DATETIME，其余的日期类型不支持。 字符串类型，如CHAR、VARCHAR、BINARY和VARBINARY。不支持BLOB和TEXT类型。 子分区子分区（subpartitioning）是在分区的基础上再进行分区，有时也称这种分区为复合分区（composite partitioning）。MySQL允许在RANGE和LIST的分区上再进行HASH或KEY的子分区。 子分区的检录需要注意一下几个问题： 每个子分区的数量必须相同； 要在一个分区表的任何分区上使用SUBPARTITION来明确定义任何子分区，就必须定义所有的子分区 每个SUBPARTITION子句必须包括子分区的一个名字 子分区的名字必须是唯一的。 子分区可以用于特别大的表，在多个磁盘减分别分配数据和索引。 分区中的NULL值MySQL数据库的分区总是NULL值小于任何的一个非NULL值。这和MySQL数据库中处理NULL值的ORDER BY操作时一样的。因此对于不同的分区类型，MySQL数据库对于NULL值得处理也是各不相同的。 对于RANGE分区，如果向分区列插入了NULL值，则MySQL数据库会将该值放入最左边的分区。如果删除了最左边的分区，那么该列为NULL值得记录也会全部被删除。 在LIST分区下要使用NULL值，必须显式地指出哪个分区中放入NULL值，否则会报错。 HASH和KEY分区对于NULL的处理方式和RANGE分区、LIST分区不同，任何分区函数都会讲含有NULL值的记录返回为0 分区和性能数据库的应用分为两类：一类是OLTP（在线事务处理），如Blog、电子商务、网络游戏等；一类是OLAP（在线分析处理），如数据仓库、数据集市等。 对于OLAP的应用，分区的确是可以很好地提高查询的性能，因为OLAP应用大多数查询需要频繁地扫描一张很大的表。 对于OLTP的应用，通常不可能会获取一张大表中10%的数据，大部分都是通过索引返回几条记录即可。而根据B+树索引的原理可知，对于一张大表，一般的B+树需要2~3次的磁盘IO，因此B+树可以很好地完成工作，不需要分区的帮助。盲目使用分区，可能会带来更多的因为分区扫描而使用的磁盘IO。 在表和分区间交换数据MySQL支持ALTER TABLE … EXCHANGE PARTITION语法。该语法允许分区或子分区中的数据与另一个非分区的表中的数据进行交换。如果非分区表中的数据为空，那么相当于将分区中的数据移动到非分区中；若分区表中的数据为空，则相当于将外部表的数据导入到分区。 要使用ALTER TABLE … EXCHANGE PARTITION语句，必须满足以下几点： 要交换的表需和分区表有着相同的表结构，但是表不能含有分区 在非分区表中的数据必须在交换的分区中定义 被交换的表中不能含有外键，或其他的表含有对该表的外键引用 用户除了需要ALTER、INSERT和CREATE权限外，还需要DROP权限 使用该语句时，不会触发交换表和被交换表上的触发器；AUTO_INCREMENT列将被重置。 语法样例：”ALTER TABLE t1 EXCHANGE PARTITION p0 WITH TABLE t2;”","link":"/2020/05/04/mysql/ch04-%E8%A1%A8/"},{"title":"ch06-锁","text":"什么是锁锁是数据库系统区别于文件系统的一个关键特性。锁机制用于管理对共享资源的并发访问。InnoDB存储引擎或在行级别上对表数据上锁。数据库系统使用锁是为了支持对共享资源进行并发访问，提供数据的完整性和一致性。 对于MyISAM引擎，其锁是表锁设计，并发情况下的读没有问题，但是并发插入时的性能就比较差。InnoDB存储引擎锁的实现和Oracle数据库非常类似，提供一致性的非锁定读、行级锁支持。 Lock和LatchLatch一般称为闩锁（轻量级锁），因为其要求锁定的时间必须非常短。在InnoDB中，latch又可以分为mutex（互斥量）和rwlock（读写锁）。其目的是用来保证并发线程操作临界资源的正确性，并且通常没有死锁检测的机制。 Lock的对象是事务，用来锁定的是数据库中的对象，如表、页、行。并且一般lock的对象仅在事务commit或rollback后进行释放（不同事务隔离级别释放的时间可能不同）。此外，正如大多数数据库一样，lock是由死锁机制的。 InnoDB存储引擎中的锁InnoDB存储引擎实现了如下两种标准的行级锁： 共享锁（S Lock）,允许事务读取一行数据。 排他锁（X Lock）,允许事务删除或更新一行数据。 如果一个事务t1已经获取了行r的共享锁，那么另外的事务t2可以立即获得行r的共享锁，因为读取并没有改变行r的状态，称这种情况为锁兼容（Lock Compatible）。但若有其他的事务t3箱获得行r的排他锁，则必须等待事务t1、t2释放行r上的共享锁，这种情况称为锁不兼容。 此外，InnoDB存储引擎支持多粒度（granular）锁定，这种锁定允许事务在行级别上的锁和表级别上的锁同时存在。为了支持不同粒度上进行加锁操作，InnoDB存储引擎支持一种额外的锁方式，称之为意向锁（Intention Lock）。意向锁是将锁定的对象分为多个层次，意向锁意味着事务希望在更细粒度（fine granularity）上进行加锁。 若将上锁的对象看成一颗树，那么对最下层的对象加锁，也就是最细粒度的对象进行上锁，那么首先需要对粗粒度的对象上锁。 InnoDB支持的意向锁为表级别的锁，设计的主要目的是为了在一个事务中揭示下一行将被请求的锁类型。其支持两种意向锁： 意向共享锁（IS Lock），事务想要获得一张表中某几行的共享锁 意向排他锁（IX Lock），事务想要获得一张表中某几行的排它锁 由于InnoDB存储引擎支持的是行级别的锁，因此意向锁其实不会阻塞除全表扫描以外的任何请求。 一致性非锁定读一致性非锁定读（consistant nonlocking read）是指InnoDB存储引擎通过多版本控制（multi versioning）的方式来读取当前执行时间数据库中行的数据。如果读取的行正在执行DELETE或UPDATE操作，这是读取操作不会因此去等待行上锁的释放，相反地，InnoDB会去读取行的一个快照数据。因为不需要等待访问的行上X锁的释放，所以称其未非锁定读。快照数据是指该行的之前版本的数据，该实现是通过undo段来完成。而undo用来在事务中回滚数据，因此快照数据本身是没有额外开销的。 非锁定读机制极大地提高了数据库的并发性，在InnoDB存储引擎的默认设置下，这是默认的读取方式，即读取不会占用和等待表上的锁。但是在不同的事务隔离级别下，读取的方式不同，并不是在每个事务隔离级别下都是采用非锁定读的一致性读。 由图可知，快照数据其实就是当前行数据之前的历史版本，每行记录可能有多个版本。一个行记录可能有不止一个快照数据，一般称这种技术为行多版本技术，由此带来的并发控制称之为多版本并发控制（Multi Version Concurrency Control， MVCC）。 在事务隔离级别为READ COMMITTED和REPATABLE READ（InnoDB存储引擎默认的事务隔离级别）下，InnoDB存储引擎使用非锁定的一致性读。然而，对于快照数据的定义却不相同。在READ COMMITTED事务隔离级别下，对于快照数据，非一致性读总是读取被锁定行的最新一份快照数据；而在REPEATABLE READ事务隔离级别下，对于快照数据，非一致性读总是读取事务开始时的行数据版本。 一致性锁定读在默认配置下，即事务的隔离级别为REPEATABLE READ模式下，InnoDB存储引擎的SELECT操作使用一致性非锁定读。但是在某些情况下，用户需要显式地对数据库读取操作进行加锁以保证数据逻辑的一致性。而这要求数据库支持加锁语句，即使是对于SELECT这类的读操作。InnoDB对于SELECT语句支持两种一致性的锁定读（locking read）操作： SELECT … FOR UPDATE SELECT … LOCK IN SHARE MODE SELECT … FOR UPDATE 对读取的行记录加一个X锁，其他事物不能对已锁定的行加上任何锁。SELECT … LOCK IN SHARE MODE对读取的行记录加S锁，其他事物可以向被锁定的行加S锁，但是如果加X锁，则会被阻塞。 SELECT … FOR UPDATE 和SELECT … LOCK IN SHARE MODE都必须在一个事务中，当事务提交了，锁也就释放了。因此在使用上述两句SELECT锁定语句时，需要加上BEGIN等加锁语句。 自增长与锁自增长在数据库中是非常常见的一种属性。在InnoDB的内存结构中，对每个含有自增长值得表都有一个自增长计数器（auto-increment counter）。当对含有自增长的计数器的表进行插入操作时，这个计数器就会被初始化，通过如下语句来得到计数器的值： 1SELECT MAX(auto_inc_col) FROM t FOR UPDATE; 插入操作会依据这个自增长的计数器值加1来赋值自增长列。这个实现方式称做AUTO-INC Locking。这种锁其实是采用一种特殊的表锁机制，为了提高插入的性能，锁不是在一个事务完成后才释放，而是在完成对自增长值插入的SQL语句后立即释放。 当前的MySQL版本中，InnoDB提供了一种轻量级互斥量的自增长实现机制，这种机制大大提高了自增长值插入的性能，并且InnoDB还提供了一个参数innodb_autoinc_lock_mode参数来控制自增长模式。 自增长的插入分为以下几类： innodb_autoinc_lock_mode参数取值如下 在InnoDB中，自增长值得列必须是索引，同时必须是索引的第一个列。 外键和锁外键主要用于引用完整性的约束检查。在InnoDB中，对于一个外键列，如果没有显式地对这个列加索引，InnoDB会自动对其加一个索引，因为这样可以避免表锁。 对于外键值得插入或更新，首先需要查询父表中的记录，即SELECT父表。但是对于父表的SELECT操作，不是使用一致性非锁定读的方式，因为这样会发生数据不一致的问题，因此这是使用的是SELECT … LOCK IN SHARE MODE方式。 锁的算法行锁的3种算法InnoDB有3中行锁的算法，分别是： Record Lock: 单个行记录上的锁 Gap Lock: 间隙锁，锁定一个范围，单不包括记录本身 Next-Key Lock: Gap Lock + Record Lock, 锁定一个范围，并且锁定记录本身 Record Lock总是会去锁住索引记录，如果InnoDB存储引擎表在建立时没有设置任何索引，那么这是InnoDB会使用隐式的主键来进行锁定。Next-Key Lock是结合了Gap Lock和Record Lock的一种锁定算法，在Next-Key Lock算法下，InnoDB对于行的查询都是采用这种锁定算法。采用Next-Key Lock的锁定技术称为Next-Key Locking.其设计的目的是为了解决Phantom Problem. 当查询的索引含有唯一属性时，InnoDB会对Next-Key Lock进行优化，将其降级为Record Lock。Gap Lock是为了阻止多个事务将记录插入到同一范围内，而这会导致Phantom Problem. 用户可以通过以下两种方式来显式地关闭Gap Lock： 将事务的隔离级别设置为READ COMMITTED 将参数innodb_locks_unsafe_for_binlog 设置为1 解决Phantom Problem在默认的事务隔离级别下，即REPEATABLE READ下，InnoDB存储引擎采用Next-key Locking机制来避免Phantom Problem。Phantom Problem是指在同一事务下，连续执行两次同样的SQL语句可能导致不同的结果，第二次的SQL语句可能返回之前不存在的行。 锁问题脏读脏数据是指事务对缓冲池中行记录的修改，并且还没有被提交。如果读到了脏数据，即一个事务可以读到另外一个事务中未提交的数据，这样就违背了数据库的隔离性。 脏读是指在不同的事务下,当前事务可以读到另外事务未提交的数据。脏读现象在生产环境中并不常见，脏读发生的条件是需要事务的隔离级别为READ UNCOMMITTED. 不可重复读不可重复读是指在一个事务内多次读取同一数据集合，在这个事务还没结束时，另外一个事务也访问该同一数据集合，并做了一些DML操作，导致在第一个事务中的两次读数据之间，由于第二个事务的修改，那么第一个事务两次读到的数据可能不一样。 不可重复度和脏读的区别是：脏读是读到未提交的数据，而不可重复读读到的确实已经提交的数据，但是其违反了数据库事务一致性的要求。 一般来说，不可重复读的问题是可以接受的，因为其读到的是已经提交的数据，本身并不会带来很大的问题。READ COMMITTED隔离级别允许不可重复读的现象。 InnoDB使用Next-Key Locking算法来避免不可重复读的问题，这个问题也叫Phantom Problem。在Next-Key Lock算法下，对于索引的扫描，不进是锁住扫描到的索引，还锁住这些索引覆盖的范围（gap）。因此在这个范围内的插入都是不允许的。这样就避免了另外的事务在这个范围内插入数据导致不可重复读的问题。因此，InnoDB默认的事务隔离级别为READ REPEATABLE。 丢失更新丢失更新是指一个事务的更新操作会被另一个事务的更新操作覆盖掉，从而导致数据的不一致，例如： 事务T1将记录r更新为v1，但是事务T1并没有提交； 与此同时，事务T2将行记录r更新为v2，事务T2未提交； 事务T1提交 事务T2提交 当前数据库的任何隔离级别都能避免丢失更新，即使是READ UNCOMMITTED的事务隔离级别，对于行的DML操作，需要对行货其它粗粒度级别的对象加锁，因此在上述例子中，事务T2并不能对记录r进行更新操作，而是被阻塞，直到事务T1提交。 在实际的生产环境中却有可能发生丢失更新的现象： 事务T1查询一行数据，放入本地内存，并显示给一个终端用户User1； 事务T2页查询该行数据，并将取得的数据显示给终端用户User2； User1修改这行记录，更新数据库并提交； User2修改这行记录，更新数据库并提交 要避免上述问题，需要让事务的操作变成串行化。即： 对用户User1读取的记录加上一个排它锁 对用户User2读取的记录加上一个排它锁 事务T1提交 事务T2提交 阻塞因为不同锁之间的兼容性关系，在有些时刻一个事务中的锁需要等待另一个事务的锁释放它所占的资源，这就是阻塞。 在InnoDB存储引擎中，参数innodb_lock_wait_timeout用来控制等待的时间（默认50s），innodb_rollback_on_timeout用来设定是否在等待超时时对进行中的事务进行回滚操作。在默认情况下，InnoDB不会回滚超时引发的错误异常。 死锁死锁是指两个或两个以上的事务在执行过程中，因争夺资源而造成的一种互相等待的现象。解决死锁最简单的办法是不要有等待，将任何的等待都转化为回滚，并且事务重新开始。解决死锁的另一种简单方法是超时，即当两个事务互相等待时，当一个等待时间超过设置的某一阈值时，其中一个事务进行回滚，另一个等待的事务就能继续执行。当前数据库普遍采用wait-for grap(等待图)的方式来进行死锁检测。较之超时的解决方案，这是一种更主动的死锁检测方式。 锁升级锁升级（Lock Escalation）是指将当前锁的粒度降低。InnoDB存储引擎不存在锁升级的问题。因为其不是根据每个记录来产生行锁的，相反，其根据每个事务访问的每个页对锁进行管理的，采用的是位图方式。因此，不管一个事务锁住页中一个记录还是多个记录，其开销通常都是一致的。","link":"/2020/05/04/mysql/ch06-%E9%94%81/"},{"title":"ch05-索引与算法","text":"InnoDB存储引擎索引概述 B+树索引 全文索引 哈希索引 InnoDB存储引擎支持的哈希索引是自适应的，InnoDB会根据表的使用情况自动为表生成哈希索引，不能人为干预是否在一张表中生成哈希索引。 B+树是传统意义上的索引。B+树索引并不能找到一个给定键值的具体行，只能找到被查找数据所在的页，然后数据库通过把页读入到内存，再在内存中进行查找，最后得到要查找的数据。 数据结构与算法因为每页Page Directory中的槽是按照主键的顺序存放的，对于某一条具体记录的查询就是通过对Page Directory进行二分查找得到的。 二叉查找树二叉查找树查询时间最大为树的高度， 在最坏情况下（退化成链表），查询时间为O(n). 为了提高查询效率，需要尽可能降低树的高度。当树是平衡树时，二叉树的查找效率是最高的，而典型的平衡二叉树就是AVL树。 平衡二叉树的定义：平衡二叉树首先是一棵二叉查找树，其次必须满足任何节点的两个子树的高度最大相差1.平衡二叉树虽然提高了查询效率，但是维护成本高。如对于插入，删除等操作，需要通过一次或多次左右旋转才能维护平衡二叉树的特性。由于平衡二叉树多用于内存结构对象中，因此可以尽量降低维护时间。 B+树B+树是为磁盘或其它直接存取辅助设备设计的一种平衡查找树。在B+树中，所有记录节点都是按键值的大小顺序存放在同一层的叶子节点上，由个叶子节点指针进行连接。 B+树的插入操作B+树的插入操作必须保证插入后叶子节点中的记录依然有序，同时需要考虑插入到B+树的三种情况： 为了保持平衡，对于新插入的键值可能需要做大量的拆分页（split）操作。因为B+树结构主要用于磁盘，页的拆分意味着磁盘的操作，所以应该在可能的情况下尽量减少页的拆分操作。因此，B+树同样提供了类似于平衡二叉树的旋转（Rotation）特性。 旋转发生在leaf Page已满，但是其左右兄弟节点没有满的情况下。这是B+树并不会急着去做拆分页的操作，而是将记录移到所在页的兄弟节点上。 B+树的删除操作B+树使用填充因子（fill factor）来控制树的删除变化，50%是填充因子可设的最小值。B+树的删除操作同样必须保证删除后叶子节点中的记录依然有序： B+树索引B+树索引的本质就是B+树在数据库中的实现。B+树在数据库中有一个特点是高扇出性，因此在数据库中，B+树的高度一般都在24层，这也就是说查找某一键值的行记录最多需要24次IO。 数据库中的B+树索引可以分为聚集索引（clustered index）和辅助索引（secondary index），但不管是聚集索引还是辅助索引，其内部都是B+树的，即高度平衡的，叶子节点存放着所有的数据。聚集索引与辅助索引不同的是，聚集索引的叶子节点存放的是一整行的信息 聚集索引InnoDB存储引擎表是索引组织表，即表中的数据按照主键的顺序存放，而聚集索引就是按照每张表的主键构造的B+树，同时叶子节点中存放的即为整张表的行记录数据，也将聚集索引的叶子节点称为数据页。聚集索引的这个特性决定了索引组织表中数据也是索引的一部分。 由于实际的数据页只能按照一颗B+树进行排序，因此每张表只能拥有一个聚集索引。在非数据页的索引页中，存放的仅仅是键值及指向数据页的偏移量，而不是完整的行记录。 聚集索引的存储并不是物理上连续的，而是逻辑上连续的。这其中有两点：一是页通过双向链表连接，页按照主键的顺序排列；而是每个页中的记录也是通过双向链表进行维护的，物理上可以同样不按照主键储存。 辅助索引对于辅助索引，叶子节点并不包含行记录的全部数据。叶子节点除了包含键值以外，每个叶子节点中的索引行中还包含了一个书签（bookmark）。该书签用来告诉InnoDB存储引擎哪里可以找到索引相对应的数据。由于InnoDB的存储引擎表是索引组织表，因此InnoDB存储引擎的辅助索引中的书签就是相应行数据的聚集索引值。 辅助索引的存在并不影响数据在聚集索引中的组织，因此每张表上可以有多个辅助索引。当通过辅助索引来寻找数据时，InnoDB存储引擎会遍历辅助索引并通过叶子节点获得指向主键索引的主键，然后再通过主键索引来找到完整的行记录。 B+树索引的管理1.索引管理 索引的创建和删除可以通过两种方法，一种是ALTER TABLE, 另一种是CREATE/DROP INDEX. Cardinality值Cardinality值表示索引中不重复记录数量的预估值。在实际应用中，Cardinality/n_rows_in_table应尽可能接近1. 全文检索全文检索（Full-Test Search）是将存储于数据库中的整本书或整篇文章中的任意内容信息查找出来的技术。它可以根据需要获得全文中有关章、节、段、句、词等信息，也可以进行各种统计和分析。 倒排索引 全文检索通过使用倒排索引（inverted index）来实现。倒排索引同B+树索引一样，也是一种索引结构。它在辅助表(auxiliary table)中存储了单词与单词自身在一个或多个文档中所在位置之间的映射。这通常利用关联数组实现，其拥有两种表现形式： inverted file index: 表现形式为{单词，单词所在文档ID} full inverted index: 表现形式为{单词，单词所咋文档ID, 在具体文档的位置} InnoDB全文检索InnoDB采用full inverted index的方式支持全文检索。","link":"/2020/05/04/mysql/ch05-%E7%B4%A2%E5%BC%95%E4%B8%8E%E7%AE%97%E6%B3%95/"},{"title":"ch08-事务","text":"事务（Transaction）是数据库区别于文件系统的重要特性之一。事务会把数据库从一种一致状态转换为另一种一致状态。在数据库提交工作时，可以确保要么所有修改都已经保存了，要么所有修改都不保存。 InnoDB存储引擎中的事务完全符合ACID的特性。ACID具体是指： 原子性（Atomicity） 一致性（Consistency） 隔离性（Isolation） 持久性（Durability） 认识事务概述事务可由一条或一组SQL组成。事务时访问并更新数据库中各种数据项的一个程序执行单元。在事务中的操作，要么都做修改，要么都不做。 原子性（Atomicity）A. 指整个数据库事务是一个不可分割的工作单元。只有使事务中所有的数据库操作都执行成功，才算整个事务成功。只有使事务中所有的数据库操作都执行成功，才算整个事务成功。事务中任何一个SQL语句执行失败，已经执行成功的SQL语句页必须撤销，数据库状态应该退回到执行事务前的状态。 一致性（Consistency）C. 一致性指事务将数据库从一种状态转变为下一种一致的状态。在事务开始之前和事务结束之后，数据库的完整性约束没有被破坏。 隔离性（Isolation）I. 事务的隔离性要求每个读写事务的对象对其他事务的操作对象能相互分离。即该事务提交前对其他事务都不可见，这通常使用锁来实现。 持久性（Durability）D. 持久性是指事务一旦提交后，其结果就是永久性的。即使发生宕机等故障，数据库也能将数据恢复。持久性保证事务系统的高可靠性，而不是高可用性。 分类事务可以分为以下几类： 扁平事务（Flat Transactions） 带有保存点的扁平事务（Flat Transactions with Savepoints） 链事务（Chained Transactions） 嵌套事务（Nested Transactions） 分布式事务（Distributed Transactions） 扁平事务是事务中最简单的一种，在扁平事务中，所有的操作都处于同一层次，其由BEGIN WORK开始，由COMMIT WORK或ROLLBACK WORK结束，期间的操作是原子的，要么都执行，要么都回滚。因此，扁平事务时应用程序称为原子操作的基本组成模块。扁平事务的主要限制是不能提交或者回滚事务的某一部分，或分几个步骤提交。 带保存点的扁平事务除了支持扁平事务支持的操作外，允许在事务执行过程中回滚到统一事务中较早的一个状态。这是因为某些事务可能在执行的过程中出现的错误并不会导致所有的操作都无效，放弃整个事务不合乎要求，开销也大。保存点（Savepoint）用来通知系统应该记住事务当前的状态，以便当之后发生错误时，事务能回滚到保存点当时的状态。 链事务可视为保存点模式的一种变种。带有保存点的扁平事务，当发生系统崩溃时，所有的保存点都将小时，因为其保存点是易失（volatile）的。链式事务的思想是：在提交一个事务时，释放不需要的数据对象，将必要的处理上下文隐式地传给下一个要开始的事务。注意，提交事务操作和开始下一个事务操作将合并为一个原子操作，这意味着下一个事务将考单上一个事务的结果，就好像在一个事务中进行一样。链式事务的回滚仅限于当前事务，即只能恢复到最近的一个保存点。 嵌套事务时一个层次结构框架。由一个顶层事务（top-level transaction）控制着各个层次的事务。顶层事务之下嵌套的事务被称为子事务（subtransaction），其控制每一个局部的变换。InnoDB不支持嵌套事务。 分布式事务通常是一个在分布式环境下运行的扁平事务，因此需要根据数据所在位置访问网络中的不同节点。 事务的实现redo log称为重做日志，用来保证事务的原子性和持久性；undo log用来保证事务的一致性，用来帮助事务回滚及MVCC功能。redo通常是物理日志，记录的是页的物理修改操作；undo是逻辑日志，根据每行记录进行记录。redo log基本上都是顺序写的，在数据库运行时不需要对redo log的文件进行读取操作；undo log是需要进行随机读写的。 redo log基本概念 重做日志用来实现事务的持久性。其由两部分组成：一是内存中的重做日志缓冲（redo log buffer），是易失的；二是重做日志文件（redo log file），是持久的。 InnoDB是事务的存储引擎，其通过Force Log at Commit机制实现事务的持久性，即当事务提交（commit）时，必须先将该事务的所有日志写入重做日志文件进行持久化，待事务的COMMIT操作完成才算完成。 为了确保每次日志都写入重做日志，在每次将重做日志缓冲写入重做日志文件后，InnoDB存储引擎都要调用一次fsync操作。因此，磁盘的吸能决定了事务提交的性能，也就是数据库的性能。 在MySQL中海油一种二进制日志（binlog），其用来进行POINT-IN-TIME（PIT）的恢复以及主从复制（Replication）环境的建立。binlog是MySQL数据库层面的，MySQL数据库中任何存储引擎对于数据库的更改都会产生二进制日志。其次，binlog是一种逻辑日志，其记录的是对应的SQL语句；binlog只在事务提交完成后进行一次写入。 lock block 在InnoDB中，重做日志都是以512字节进行存储的，这意味着重做日志缓存，重做日志文件都是以块（block）的方式进行保存的，称之为重做日志块（redo log block），每块的大小为512字节。由于重做日志块的大小和磁盘扇区大小一样，重做日志的写入可以保证原子性，不需要doublewrite技术。 log group log group为重做日志组，其中有多个重做日志文件。log group是一个逻辑上的概念，并没有一个世纪存储的物理文件来表示log group信息。 重做日志格式 由于InnoDB存储引擎的存储管理是基于页的，重做日志格式也是基于页的。 恢复 InnoDB存储引擎在启动时不管上次数据库运行时是否正常关闭，都会尝试进行恢复操作。因为重做日志记录的是物理日志，因此恢复的速度比逻辑日志要快得多。 undo log基本概念 当事务需要回滚时，就要用到undo log了。在对数据库进行修改时，InnoDB存储引擎不但会产生redo，还会产生一定量的undo。这样如果用户执行的事务或语句由于某种原因失败了，又或者用户用一条ROLLBACK语句请求回滚，就可以利用这些undo信息进行回滚了。 undo log存放在数据库内部的一个特殊段（segment）中，这个段称为undo 段（undo segment）。undo段位于共享表空间内。undo是逻辑日志，因此只是将数据库逻辑地恢复到原来的样子。所有修改都被逻辑地取消了，但是数据结构和页本身在回滚之后可能大不相同。这是因为在多用户并发系统中，可能会有数十、数百甚至数千个并发事务。当一个事务在修改当前一个页中某几条记录时，同时还有其他事务在同一个页中修改另几条记录，这时就不能将一个页回滚到事务开始的样子，因为这样会影响其他事务正在进行的工作。 当InnoDB回滚时，它实际上做的是与之前相反的工作，如对DELETE操作，执行一个INSERT操作等。 除了回滚操作，undo的另一个作用是MVCC，即在InnoDB存储引擎中MVCC的实现是通过undo来完成的。当用户读取一行记录时，若该记录已经被其他事务占用，当前事务可以通过undo读取之前的版本信息，以此实现非锁定读取。 undo log会产生redo log undo存储管理 InnoDB存储引擎有rollback segment，每个回滚段记录了1024个undo log segment，而在每个undo log segment中进行undo页的申请。 事务在undo log segment分配页并写入undo log的这个过程同样需要写入重做日志。当事务提交时，InnoDB存储引擎会做以下两件事情： 将undo log放入列表中，以供之后的purge操作； 判断undo log所在的页是否可以重用，若可以分配给下个事务使用 purge delete和update操作可能并不是直接删除或更新原有的数据，而是在purge操作中被“延时”完成了。purge用于最终完成delete和update操作，这样设计是因为InnoDB支持MVCC，所以记录不能在事务提交时立即进行处理。这时其他事务可能正在引用这行，故InnoDB存储引擎需要保存记录之前的版本。purge判断行记录不被任何其他事务引用，才会进行真正的delete操作。 group commit 若事务未非只读事务，则每次事务提交时需要进行一次fsync操作，以此保证重做日志都已经写入磁盘。为了提高磁盘fsync的效率，当前数据库都提供了group commit的功能，即一次fsync可以刷新确保多个事务日志被写入文件，对于InnoDB存储引擎来说，事务提交时会进行两个阶段的操作： 修改内存中事务对应的信息，并且将日志写入重做日志缓冲 调用fsync将确保日志都从重做日志缓冲写入磁盘 group commit是指在步骤2中，可以将多个事务的重做日志通过一次fsync刷新到磁盘。 事务的隔离级别SQL标准定义的四个隔离级别为： READ UNCOMMITTED READ COMMITTED(Phantom Problem) READ REPEATABLE SERIALIZABLE 分布式事务MySQL数据库分布式事务InnoDB存储引擎提供了对XA事务的支持，并通过XA事务来支持分布式事务的实现。分布式事务指的是允许多个独立的事务资源（transactional resources）参与到一个全局的事务中。全局事务要求在其中的所有参与的事务要么都提交，要么都回滚。在使用分布式事务时，InnoDB存储引擎的事务隔离级别必须设置为SERIALIZABLE。 XA事务允许不同数据库之间的分布式事务，如一台服务器是MySQL数据库，另一台是Oracle数据库，还有一台是SQL Server数据库，只要参与在全局事务中的每个节点都支持XA事务。分布式事务可能在银行系统的转账中比较常见。 XA事务由一个或多个资源管理器（Resource Managers）、一个事务管理器（Transaction Manager）以及一个应用程序（Application Program）组成。 资源管理器：提供访问事务资源的方法，通常一个数据库就是一个资源管理器； 事务管理器：协调参与全局事务中的各个事务。需要和参与全局事务的所有资源管理器进行通信。 应用程序：定义事务的边界，指定全局事务中的操作。 在MySQL数据库的分布式事务中，资源管理器就是MySQL数据库，事务管理器为连接MySQL服务器的客户端。 分布式事务使用两段式提交（two-phrase commit）的方式。在第一阶段，所有参与全局事务的节点都开始准备（PREPARE），告诉事务管理器它们都准备好了；在第二阶段，事务管理器告诉资源管理器执行ROLLBACK还是COMMIT。如果任何一个节点显式不能提交，则所有的节点都被告知需要回滚。与本地事务不同的是，分布式事务需要多一次PREPARE操作，待收到所有节点的同意信息后，再进行COMMIT或是ROLLBACK操作。 内部XA事务在MySQL数据库内部还存在一种分布式事务，其在存储引擎与存储引擎之间，称之为内部XA事务。 最为常见的内部XA事务存在于binlog与InnoDB之间，由于复制的要求，因此目前绝大多数的数据库都开启了binlog功能。在事务提交时，先写二进制日志，再写InnoDB的重做日志。这两个操作要求是原子的，否则如果二进制日志先写了，而在写入InnoDB存储引擎时发生了宕机，那么slave可能会收到master传过去的二进制日志并执行，最终导致了主从不一致的情况。为了解决这个问题，MySQL数据库在binlog与InnoDB存储引擎之间采用了XA事务。当事务提交时，InnoDB会先做一个PREPARE操作，将事务的xid写入，接着进行二进制日志的写入，如果在InnoDB存储引擎提交前，MySQL数据库宕机了，那么MySQL数据库在重启后会先检查准备的UXID是否提交，若没有，则在存储引擎层再进行一次提交操作。 不好的事务习惯在循环中提交使用自动提交使用自动回滚长事务长事务就是执行时间较长的事务。对于长事务，在执行过程中，当数据库或操作系统，硬件等发生问题时，重新开始事务的代价变得不可接受。数据库需要回滚所有已经发生的变化，而这个过程可能比产生这些变化的时间还长。因此，对于长事务，有时可以通过转化为小批量的事务来进行处理。当事务发生错误时，只需要回滚一部分数据，然后接着上次已完成的事务继续进行。","link":"/2020/05/04/mysql/ch08-%E4%BA%8B%E5%8A%A1/"},{"title":"HashMap in Java 8","text":"重要字段12345678910111213141516171819202122232425262728293031323334353637383940/** * 默认容量 - MUST be a power of two. */static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; // aka 16/** * 最大容量 * MUST be a power of two &lt;= 1&lt;&lt;30. */static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30;/** * 无参数构造器的默认负载因子 */static final float DEFAULT_LOAD_FACTOR = 0.75f;/** * The bin count threshold for using a tree rather than list for a * bin. Bins are converted to trees when adding an element to a * bin with at least this many nodes. The value must be greater * than 2 and should be at least 8 to mesh with assumptions in * tree removal about conversion back to plain bins upon * shrinkage. */static final int TREEIFY_THRESHOLD = 8;/** * The bin count threshold for untreeifying a (split) bin during a * resize operation. Should be less than TREEIFY_THRESHOLD, and at * most 6 to mesh with shrinkage detection under removal. */static final int UNTREEIFY_THRESHOLD = 6;/** * The smallest table capacity for which bins may be treeified. * (Otherwise the table is resized if too many nodes in a bin.) * Should be at least 4 * TREEIFY_THRESHOLD to avoid conflicts * between resizing and treeification thresholds. */static final int MIN_TREEIFY_CAPACITY = 64; 基本存储单元1234567891011121314151617181920212223242526272829303132333435363738394041424344454647/** * Basic hash bin node, used for most entries. (See below for * TreeNode subclass, and in LinkedHashMap for its Entry subclass.) */static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; { final int hash; final K key; V value; Node&lt;K,V&gt; next; Node(int hash, K key, V value, Node&lt;K,V&gt; next) { this.hash = hash; this.key = key; this.value = value; this.next = next; } public final K getKey() { return key; } public final V getValue() { return value; } public final String toString() { return key + \"=\" + value; } // 重写了hashCode方法 public final int hashCode() { return Objects.hashCode(key) ^ Objects.hashCode(value); } public final V setValue(V newValue) { V oldValue = value; value = newValue; return oldValue; } // 重写了equals方法 public final boolean equals(Object o) { if (o == this) // 指向同一个node return true; if (o instanceof Map.Entry) { Map.Entry&lt;?,?&gt; e = (Map.Entry&lt;?,?&gt;)o; // node相同的条件是key和val都相同 if (Objects.equals(key, e.getKey()) &amp;&amp; Objects.equals(value, e.getValue())) return true; } return false; }} 插入操作put(K key, V value)的实现如下: 123public V put(K key, V value) { return putVal(hash(key), key, value, false, true);} 可以发现是先计算key的hash值，然后通过putVal方法真正执行插入操作的，hash(Object key)的实现如下: 1234567891011121314151617181920/** * Computes key.hashCode() and spreads (XORs) higher bits of hash * to lower. Because the table uses power-of-two masking, sets of * hashes that vary only in bits above the current mask will * always collide. (Among known examples are sets of Float keys * holding consecutive whole numbers in small tables.) So we * apply a transform that spreads the impact of higher bits * downward. There is a tradeoff between speed, utility, and * quality of bit-spreading. Because many common sets of hashes * are already reasonably distributed (so don't benefit from * spreading), and because we use trees to handle large sets of * collisions in bins, we just XOR some shifted bits in the * cheapest possible way to reduce systematic lossage, as well as * to incorporate impact of the highest bits that would otherwise * never be used in index calculations because of table bounds. */static final int hash(Object key) { int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);} 这个实现要比java7简单多了。because we use trees to handle large sets of collisions in bins, we just XOR some shifted bits in the cheapest possible way to reduce systematic lossage, as well as to incorporate impact of the highest bits that would otherwise never be used in index calculations because of table bounds。这句话的意思是因为java8引入了红黑树来处理桶的大规模的碰撞，所以只是将hashCode的高位移位后进行XOR操作来减少对称损失，这是一种最简单的操作（降低hash的计算成本），同时也解决了tab有限的情形下，高位bit不会用于index计算的情况。什么是对称损失？？？ put(int hash, K key, V val, boolean onlyIfAbsent, boolean evict)的实现如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152/** * Implements Map.put and related methods. * * @param hash hash for key * @param key the key * @param value the value to put * @param onlyIfAbsent if true, don't change existing value * @param evict if false, the table is in creation mode. * @return previous value, or null if none */final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) { Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); else { Node&lt;K,V&gt; e; K k; if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); else { for (int binCount = 0; ; ++binCount) { if ((e = p.next) == null) { p.next = newNode(hash, key, value, null); if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; } if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; } } if (e != null) { // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; } } ++modCount; if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null;} 判断table是否为空，如果为空，则先分配空间 计算数组索引，并判断当前索引位置是否为null，为null的话直接新建node，不为null的话，先判断key是否存在，如果存在，则更新value，并返回旧value。否则，新建节点。 可以发现，插入操作的主逻辑大体和java7一样，只是在判断key是否存在时存在比较大的不同，这是因为引入和红黑树的原因。在java8中，判断节点是否存在需要进行以下三个步骤： 数组索引处的第一个元素的hash值以及key就和要插入的key相同，则要插入的key已经存在于hashmap中 如果数组索引处的第一个元素时TreeNode，即当前是使用红黑树进行插入的，通过putTreeVal()进行插入新的键值对 如果是使用链表存储，则在链表的查询过程中如果找到key，就进行更新；如果找不到，将键值对插入到链表的最后面，这与java7是不一样的。插入完毕后，还要判断当前桶的大小是否达到了需要树化的阈值，如果达到了还需要通过treeifyBin(tab, hash)方法进行树化。 可以看到，插入操作会引起两个额外的重要操作，一个是扩容resize，另一个是树化treeify. resize的实现如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273final Node&lt;K,V&gt;[] resize() { Node&lt;K,V&gt;[] oldTab = table; int oldCap = (oldTab == null) ? 0 : oldTab.length; int oldThr = threshold; int newCap, newThr = 0; if (oldCap &gt; 0) { if (oldCap &gt;= MAXIMUM_CAPACITY) { threshold = Integer.MAX_VALUE; return oldTab; } else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) newThr = oldThr &lt;&lt; 1; // double threshold } else if (oldThr &gt; 0) // initial capacity was placed in threshold newCap = oldThr; else { // zero initial threshold signifies using defaults newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); } if (newThr == 0) { float ft = (float)newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); } threshold = newThr; @SuppressWarnings({\"rawtypes\",\"unchecked\"}) Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; table = newTab; if (oldTab != null) { for (int j = 0; j &lt; oldCap; ++j) { Node&lt;K,V&gt; e; if ((e = oldTab[j]) != null) { oldTab[j] = null; if (e.next == null) newTab[e.hash &amp; (newCap - 1)] = e; else if (e instanceof TreeNode) ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); else { // preserve order Node&lt;K,V&gt; loHead = null, loTail = null; Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; do { next = e.next; if ((e.hash &amp; oldCap) == 0) { if (loTail == null) loHead = e; else loTail.next = e; loTail = e; } else { if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; } } while ((e = next) != null); if (loTail != null) { loTail.next = null; newTab[j] = loHead; } if (hiTail != null) { hiTail.next = null; newTab[j + oldCap] = hiHead; } } } } } return newTab;} treeify的实现逻辑如下 123456789101112131415161718192021222324/** * Replaces all linked nodes in bin at index for given hash unless * table is too small, in which case resizes instead. */final void treeifyBin(Node&lt;K,V&gt;[] tab, int hash) { int n, index; Node&lt;K,V&gt; e; if (tab == null || (n = tab.length) &lt; MIN_TREEIFY_CAPACITY) resize(); else if ((e = tab[index = (n - 1) &amp; hash]) != null) { TreeNode&lt;K,V&gt; hd = null, tl = null; do { TreeNode&lt;K,V&gt; p = replacementTreeNode(e, null); if (tl == null) hd = p; else { p.prev = tl; tl.next = p; } tl = p; } while ((e = e.next) != null); if ((tab[index] = hd) != null) hd.treeify(tab); }} 删除操作删除操作的主逻辑如下: 1234567891011121314/** * Removes the mapping for the specified key from this map if present. * * @param key key whose mapping is to be removed from the map * @return the previous value associated with &lt;tt&gt;key&lt;/tt&gt;, or * &lt;tt&gt;null&lt;/tt&gt; if there was no mapping for &lt;tt&gt;key&lt;/tt&gt;. * (A &lt;tt&gt;null&lt;/tt&gt; return can also indicate that the map * previously associated &lt;tt&gt;null&lt;/tt&gt; with &lt;tt&gt;key&lt;/tt&gt;.) */public V remove(Object key) { Node&lt;K,V&gt; e; return (e = removeNode(hash(key), key, null, false, true)) == null ? null : e.value;} 可以发现是通过removeNode(hash(key), key, null, false, true))实现的，具体逻辑如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950/** * Implements Map.remove and related methods. * * @param hash hash for key * @param key the key * @param value the value to match if matchValue, else ignored * @param matchValue if true only remove if value is equal * @param movable if false do not move other nodes while removing * @return the node, or null if none */final Node&lt;K,V&gt; removeNode(int hash, Object key, Object value, boolean matchValue, boolean movable) { Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, index; if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (p = tab[index = (n - 1) &amp; hash]) != null) { Node&lt;K,V&gt; node = null, e; K k; V v; if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) node = p; else if ((e = p.next) != null) { if (p instanceof TreeNode) node = ((TreeNode&lt;K,V&gt;)p).getTreeNode(hash, key); else { do { if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) { node = e; break; } p = e; } while ((e = e.next) != null); } } if (node != null &amp;&amp; (!matchValue || (v = node.value) == value || (value != null &amp;&amp; value.equals(v)))) { if (node instanceof TreeNode) ((TreeNode&lt;K,V&gt;)node).removeTreeNode(this, tab, movable); else if (node == p) tab[index] = node.next; else p.next = node.next; ++modCount; --size; afterNodeRemoval(node); return node; } } return null;} 对于链表中节点的移除比较简单，直接将要移除的node的父节点的直接点指向node节点的子节点即可，对于红黑树节点的移除则通过removeTreeNode(this, tab, movable)方法实现，红黑树的逻辑比较复杂，后续单独再写。","link":"/2020/05/04/leetcode/HashMap-in-Java-8/"},{"title":"ch02-InnoDB存储引擎","text":"InnoDB体系架构InnoDB存储引擎有多个内存块，可以认为这些内存组成了一个大的内存池，负责如下工作： 维护所有进程/线程需要访问的多个内部数据结构 缓存磁盘上的数据，方便快速地读取，同时在对磁盘文件的数据修改之前在这里缓存 重做日志（redo log）缓冲。 …… 后台线程的主要作用是负责刷新内存池中的数据，保证缓冲池中的内存缓存的是最近的数据，此外将已修改的数据文件刷新到磁盘文件，同时保证在数据库发生异常的情况下InnoDB能恢复到正常运行状态。 后台线程 Master Thread。Master Thread是一个非常核心的后台线程，主要负责将缓冲池中的数据异步刷新到磁盘，保证数据的一致性，包括脏页的刷新、合并插入缓冲（INSERT BUFFER）、UNDO页的回收等。每秒一次的操作包括： 日志缓冲刷新到磁盘，即使这个事务还没有提交（总是）； 合并插入缓冲（可能）； 刷新缓冲池中的脏页到磁盘（可能）； 如果当前没有用户活动，则切换到background loop（可能） 每十秒的操作包括： IO操作操作不频繁的情况下，刷新脏页到磁盘（可能）； 合并插入缓冲（总是）； 将日志缓冲刷新到磁盘（总是）； 删除无用的undo页（总是）； IO操作频繁的情况下，刷新脏页到磁盘（总是）。 IO Thread. 在InnoDB中大量使用了AIO（async IO）来处理IO请求。IO Thread的工作主要是负责这些IO请求的回调（call back）处理。 Purge Thread. 事务被提交后，其所使用的undolog可能不再需要，因此需要Purge Thread来回收已经使用并分配的undo页。由于Purge Thread需要离散地读取undo页，使用多个线程能更进一步利用磁盘的随机读写性能。 Page Cleaner Thread. 作用是将脏页的刷新操作都放到单独的线程中完成，目的也是为了减轻原Master Thread的工作及对于用户查询线程的阻塞，进一步提高InnoDB存储引擎的性能。 内存InnoDB存储引擎是基于磁盘储存的，由于CPU速度与磁盘速度之间的鸿沟，基于磁盘的数据库通常使用缓冲池技术来提高数据库的整体性能。 在数据库中进行读取页的操作，首先将从磁盘读到的页存放在缓冲池中，这个过程称为将页“FIX”在缓冲池中。下一次再读相同的页时，首先判断该页是否在缓冲池中。若在缓冲池中，称该页在缓冲池中被命中，直接读取该页；否则，读取磁盘上的页。对于数据库中页的修改，则首先修改在缓冲池中的页，然后再以一定的频率刷新到磁盘上，参考Checkpoint机制。缓冲池的大小可以通过innodb_buffer_pool_size参数配置，这个参数很重要！！！ 缓冲池中存放的数据页类型有：索引页、数据页、undo页、插入缓冲（insert buffer）、自适应哈希索引（adaptive hash index）、InnoDB存储的锁信息（lock info）、数据字典信息（data dictionary）等。缓冲池的结构如下： LRU List、Free List和Flush List通常来说，数据库中的缓冲池是通过LRU（Least Recently Used）算法来进行管理的。在InnoDB中，缓冲池中页的大小默认为16KB，同样使用LRU算法对缓冲池进行管理。InnoDB加入了midpont位置，新读入的页放入到LRU列表的midpoint位置。若直接将读取到的页放入到LRU的首部，那么某些SQL操作可能会使缓冲池中的页被刷新出，从而影响缓冲池的效率。常见的操作为索引或数据的扫描操作。 Free List: 当innodb启动时，初始化完成的所有的block（包含page的结构体）都存放在Free List中。 LRU列表用于管理已经读取的页，当数据库刚启动时，LRU列表是空的，这时页都存放在Free列表中，当需要从缓冲池中分页时，首先从Free列表中查找是否有可用的空闲页，若有，则将该页从Free列表中删除，放入到LRU列表中；否则，根据LRU算法，淘汰LRU列表末尾的页，将该内存空间分配给新的页。 page made young：页从LRU列表的old部分加入到new部分。 page not made young：因为innodb_old_blocks_time的设置而导致页没有从old部分移动到new部分的操作。 脏页（dirty page）：在LRU列表中的页被修改后称为脏页。数据库通过CheckPoint机制将脏页刷新回磁盘，Flush列表中的页即为脏页列表。脏页既存在于LRU列表中，也存在于Flush列表中。 重做日志缓冲数据库的修改操作，InnoDB首先将重做日志信息先放到这个缓冲区，然后按一定频率将其刷新到重做日志文件。由于每一秒都会将重做日志缓冲刷新到日志文件，因此用户只需要保证每秒产生的事务量在这个缓冲大小之内即可。重做日志缓冲会在下列三种情况下将内容刷新到外部磁盘的文件中： Master Thread每一秒将重做日志缓冲刷新到重做日志文件； 每个事务提交时会将重做日志缓冲刷新到重做日志文件； 当重做日志缓冲池剩余空间不足1/2时，重做日志缓冲刷新到重做日志文件。 额外的内存池在InnoDB存储引擎中，对内存的管理是通过一种称为内存堆（heap）的方式进行的。在对一些数据结构本身的内存进行分配时，需要从额外的内存池中进行申请，当该区域的内存不足时，会从缓冲池中进行申请。 Checkpoint技术为了避免发生数据丢失的问题，当前事务数据库系统普遍都采用了Write Ahead Log策略，即当事务提交时，先写重做日志，再修改页。当由于发生宕机而导致数据丢失时，通过重做日志来完成数据恢复。这也是事务ACID中D（Durability）的要求。Checkpoint技术的目的是解决以下几个问题： 缩短数据库的恢复时间； 缓冲池不够用时，将脏页刷新到磁盘； 重做日志不可用时，刷新脏页。 当缓冲池不够用时，根据LRU算法会溢出最近最少使用的页，若此页为脏页，那么需要强制执行Checkpoint，将脏页刷回磁盘。 在InnoDB内部，有两种Checkpoint，分别为： Sharp Checkpoint Fuzzy Checkpoint Sharp Checkpoint发生在数据库关闭时将所有的脏页都刷新回磁盘，默认的工作方式。InnoDB存储引擎内部使用Fuzzy Checkpoint进行页的刷新，即只刷新一部分脏页，而不是刷新所有的脏页回磁盘。InnoDB可能会发生如下几种情况的Fuzzy Checkpoint: Master Thread Checkpoint: 差不多以每秒或每十秒的速度从缓冲池中的脏页列表中刷新一定比例的页回磁盘，这个过程是异步的，即此时InnoDB存储引擎可以进行其他操作，不会阻塞用户查询。 FLUSH_LRU_LIST Checkpoint: InnoDB需要保证LRU列表需要有差不多100个空闲页可用。检查LRU列表是否有足够的可用空间在Page Cleaner线程中进行，是否会阻塞用户查询？？？ Async/Sync Flush Checkpoint: 重做日志文件不可用时，需要强制将一些页刷新回磁盘，而此时脏页是从脏页列表中选取的。Page Cleaner Thread中实现，不会阻塞用户查询线程。 Dirty Page too much Checkpoint: 保证缓冲池中有足够可用的页。 InnoDB关键特性InnoDB存储引擎的关键特性包括： 插入缓冲（Insert Buffer） 两次写（Double Write） 自适应哈希索引（Adaptive Hash Index） 异步IO（Async IO） 刷新邻接页（Flush Neighbor Page） 插入缓冲在InnoDB存储引擎中，主键是行唯一的标识符。通常应用程序中行记录的插入顺序是按照主键递增的顺序进行插入的，因此，插入聚集索引（Primary Key）一般是顺序的，不需要磁盘的随机读取。当表结构有聚集索引和非聚集的辅助索引（secondary index）时，对于插入操作，数据页的存放还是按主键进行顺序存放的，但是对于非聚集索引叶子节点的插入不再是顺序的了，这时就需要离散地访问非聚集索引页。由于随机读取的存在而导致了插入操作性能下降，可以归结于B+树的特性决定了非聚集索引插入的离散性。 InnoDB对于非聚集索引的插入或更新操作，不是每一次直接插入到索引页中，而是先判断插入的非聚集索引页是否在缓冲池中，若在，则直接插入；若不在，则先放入到一个Insert Buffer对象中。好像是数据库这个非聚集的索引已经插入到叶子节点了，而实际并没有，只是存放在另一个位置了，然后再以一定的频率和情况进行Insert Buffer和辅助索引子节点的merge（合并）操作，这是通常能将多个插入合并到一个操作中（因为在一个索引页中），这就大大提高了对于非聚集索引插入的性能。 Insert Buffer的使用需要同时满足一下两个条件： 索引是辅助索引 索引不是唯一的 辅助索引不能是唯一的，因为在插入缓冲中，数据库并不去查找索引页来判断插入的记录的唯一性。如果去查找，则有离散读取的情况发生，从而导致Insert Buffer失去意义。 Insert Buffer的数据结构是一颗B+树，负责对所有的表的辅助索引进行Insert Buffer。这棵B+树存放在共享表空间中。因此，试图通过独立表空间ibd文件恢复表中数据时，往往导致CHECK TABLE失败。这是因为表的辅助索引中的数据可能还在Insert Buffer中，也就是共享表空间中，所以通过ibd文件进行恢复后，还需要进行REPAIR TABLE操作来重建表上所有的辅助索引。 Insert Buffer的非叶节点存放的是查询的search key（键值），如图所示： search key一共占9个字节，其中space表示待插入记录所在表的表空间id，在InnoDB存储引擎中，每个表有一个唯一的space id，可以通过space id得知是那张表。space占用4个字节，marker占用一个字节，用来兼容老版本的Insert Buffer，offset表示页所在的偏移量，占用4字节。 Insert Buffer叶子节点中的space、marker、offset字段和之前非叶节点的含义相同，如图所示： metadata字段占用4个字节，结构如下： IBUF_REC_OFFSET_COUNT用来排序每个记录进入Insert Buffer的顺序，通过这个顺序回放（replay）才能得到记录的正确位置。因为启用Insert Buffer索引后，辅助索引页（space， offset）中的记录可能被插入到Insert Buffer B+树中，所以为了保证每次Merge Insert Buffer页必须成功，还需要有一个特殊的页用来标记每个辅助索引页的可用空间，这个页的类型为Insert Buffer Bitmap。每个Insert Buffer Bitmap页用来追踪16384个辅助索引页，也就是256个区（Extent）。每个辅助索引页在Bitmap页中占用4位（bit），结构如图所示： Merge Insert Buffer当需要实现插入记录的辅助索引页不在缓冲池中时，需要将辅助索引记录首先插入到B+树中，但是Insert Buffer中的记录何时合并（Merge）到真正的辅助索引中呢？ Merge Insert Buffer的操作可能发生在以下几种情况下： 辅助索引页被读取到缓冲池中；此时需要检查Insert Buffer Bitmap页，然后确认该辅助索引页是否有记录存放于Insert Buffer B+树中。若有，则将Insert Buffer B+树中该页的记录插入到该辅助索引页中。 Insert Buffer Bitmap页追踪到该辅助索引页已无可用空间时；如插入辅助索引记录时检测到插入记录后可用空间会小于1/32，则会强制进行一次合并操作，即强制读取辅助索引页，将Insert Buffer B+树中该页的记录及待插入的记录插入到辅助索引页中 Master Thread。每次merge操作的页的数量不等。 当一个辅助索引要插入到页（space， offset）时，如果这个页不在缓冲池中，那么InnoDB存储引擎首先根据一定规则构造一个search key，接下来查询Insert Buffer这颗B+树，然后再将这条记录插入到Insert Buffer B+树的叶子节点中。 Change BufferChange Buffer适用的对象也是非唯一的辅助索引，用于对DML操作——INSERT、DELETE、UPDATE都进行缓冲，他们分别是：Insert Buffer、Delete Buffer、Purge Buffer，可视作是Insert Buffer的升级。例如，对一条记录进行UPDATE操作可能分为两个过程： 将记录标记为已删除 真正将记录删除 因此，Delete Buffer对应UPDATE操作的第一个过程，即将记录标记为删除。Purge Buffer对应UPDATE操作的第二个过程，即将记录真正删除。 两次写double write带给InnoDB存储引擎的特性是数据页的可靠性。 重做日志中记录的是对页的物理操作，如偏移量800，写’aaa’记录。但是当这个物理页本身已经发生了损坏，再对其进行重做是没有意义的。在应用重做日止之前，用户需要一个页的副本，当写入失效发生时，先通过页的副本还原该页，再进行重做，这就是double write。如图所示： double write由两部分组成，一部分是内存中的double write buffer，大小为2MB；另一部分是磁盘上共享表空间中连续的128个页，即2个区（extent），大小同样为2MB. 在对缓冲池的脏页进行刷新时，并不直接写磁盘，而是通过memcpy函数将脏页先复制到内存中的double write buffer，之后通过double write buffer 再分两次，每次1MB顺序地写入共享表空间的物理磁盘上，然后马上调用fsync函数，同步磁盘，开销小。在完成double write页的写入后，再将double write buffer中的页写入各个表空间文件中，此时的写入则是离散的，开销大。 自适应哈希索引InnoDB会监控对表上各索引页的查询，如果观察到建立哈希索引可以带来速度提升，则建立哈希索引，称之为自适应哈希索引（Adaptive Hash Index AHI）。AHI是通过缓冲池中的B+树页构造而来的，因此建立的速度很快，而却不需要对整张表构建哈希索引。AHI有一个要求，即对这个也的连续访问模式必须是一样的。 异步IO为了提高磁盘操作性能，当前的数据库系统都采用异步IO的方式来处理磁盘操作。AIO的一个优势是用户可以在发出一个IO请求后立即再发出另一个IO请求，当全部IO请求发送完毕后，等待所有的IO操作的完成；AIO的另一个优势是可以进行IO Merge操作。在InnoDB存储引擎中，read ahead方式的读取都是通过AIO完成，脏页的刷新，即磁盘的写入操作则全部由AIO完成。 刷新邻接页当刷新一个脏页时，InnoDB存储引擎会检测该页所在区（extent）的所有页，如果是脏页，那么一起进行刷新。这样可以通过AIO将多个IO写入合并为一个IO操作，故该工作机制在传统机械磁盘下有着显著的优势。 启动、关闭与恢复在关闭时，参数innodb_fast_shotdown影响着表的存储引擎为InnoDB的行为，该参数可取值为0，1，2. 默认为1. 0: 表示在MySQL关闭时，InnoDB需要完成所有的full purge和merge insert buffer，并且将所有的脏页刷新回磁盘。 1：表示不需要完成上述的full purge和merge insert buffer操作，但是在缓冲池中的一些数据脏页还是要刷新回磁盘 2：表示不完成full purge和merge insert buffer操作，也也不将缓冲池中的脏页写回磁盘，而是将日志都写入日志文件，在下次MySQL数据库启动时，进行恢复操作。 参数innodb_force_recovery影响了InnoDB存储引擎恢复的状况。该参数默认为0，代表当发生需要恢复时，进行所有的恢复操作，当不能进行有效恢复时，如数据页发生了corruption，MySQL数据库可能发生宕机，并把错误写入错误日志中。有些时候可以不用强制恢复所有数据，用户可以手动恢复数据。因为恢复所有数据时间可能很长。","link":"/2020/05/04/mysql/ch02-InnoDB%E5%AD%98%E5%82%A8%E5%BC%95%E6%93%8E/"},{"title":"ch09-备份与恢复","text":"备份与恢复概述根据不同的类型来划分备份方法： Hot Backup（热备份） Cold Backup（冷备份） Warm Backup（温备份） Hot Backup是指数据库运行中直接备份，对正在运行的数据库操作没有任何影响。Cold Backup是指备份操作在数据库停止的情况下，这种备份最为简单，一把只需要复制相关的数据库物理文件即可。Warm Backup备份同样是在数据库运行中进行的，但是会对当前数据库的操作有所影响，如加一个全局读锁来保证备份数据的一致性。 按照备份后文件的内容，备份可分为： 逻辑备份 裸文件备份 逻辑备份是指备份出的文件内容是可读的，一般是文本文件，内容由一条条SQL组成。这类方法的好处是可以观察导出文件的内容，一般适用于数据库的升级、迁移等工作，缺点是恢复时间往往较长。裸文件备份是指复制数据库的物理文件，既可以是在数据库运行中的复制，也可以是在数据库停止运行时的直接的数据文件复制。这类备份时间往往较逻辑备份短。 按照备份数据的内容来分，备份又可分为： 完全备份 增量备份 日志备份 完全备份是指对数据库进行一个完整的备份。增量备份是指在上次完全备份的基础上，对于更改的数据进行备份。日志备份主要是指对MySQL数据库二进制日志进行备份，通过对一个完全备份进行二进制日志日志的重做（replay）来完成数据库的point-in-time的恢复工作。MySQL数据库复制（reoplication）的原理就是异步实时地将二进制日志重做传送并应用到从（slave）数据库。 任何时候都需要做好远程异地备份，也就是容灾的防范。只是同一机房的两台服务器的备份是远远不够的。 冷备份对于InnoDB存储引擎的冷备份非常简单，只需要备份MySQL数据库的frm文件，共享表空间文件，独立表空间文件（*.ibd），重做日志文件。另外，建议定期备份MySQL数据库的配置文件my.cnf,这样有利于恢复操作。 在同一台机器上对数据库进行冷备份是远远不够的，至少还需要将本地产生的备份数据放到一台远程的服务器中，确保不会因为本地数据库的宕机而影响备份文件的使用。 冷备份的优点： 备份简单，只需要复制相关文件即可 备份文件易于在不同操作系统，不同MySQL版本上进行恢复 恢复相当简单，只需要把文件恢复到指定位置即可 恢复速度快，不需要执行任何SQL语句，也不需要重建索引 冷备份的缺点： InnoDB存储引擎的冷备份文件通常比逻辑文件大很多，因为表空间存放着很多其他的数据，如undo段，插入缓冲等信息 冷备份也不总是可以轻易跨平台。操作系统、MySQL版本，文件大小写敏感和浮点数格式都会成为问题 逻辑备份mysqldump mysqldump通常用来完成转存（dump）数据库的备份及不同数据库之间的移植 SELECT … INTO OUTFILE SELECT … INTO OUTFILE语句也是一种逻辑备份方法，更准确地说是导出一张表中的数据 逻辑备份的恢复 mysqldump的恢复操作比较简单，因为备份文件就是导出的SQL语句，一般只需要执行这个文件就可以了。 二进制日志备份与恢复对于InnoDB存储引擎，推荐的二进制日志的服务器配置为: log-bin=mysql-bin sync_binlog=1 innodb_support_xa=1 在备份二进制日志文件前，可以通过FLUSH LOGS命令来生成一个新的二进制日志文件，然后备份之前的二进制文件。 恢复二进制日志可以通过mysqlbinlog进行恢复。 热备份ibbackup ibbackup是InnoDB存储引擎官方提供的热备份工具，可以同时备份MyISAM和InnoDB存储引擎表。备份流程如下： 记录备份开始时，InnoDB存储引擎重做日志文件检查点的LSN 复制共享表空间文件以及独立表空间文件 记录复制完表空间文件后，InnoDB存储引擎重做日志的检查点LSN 复制在备份时产生的重做日志 XtraBackup XtraBackup实现增量备份 快照备份MySQL并不支持快照备份，因此快照备份是指通过文件系统支持的快照功能对数据库进行备份。 复制复制的工作原理复制是MySQL数据库提供的一种高可用高性能的解决方案，一般用来建立大型的应用。总体来说，relication的工作原理分为以下3个步骤： 主服务器（master）把数据更改记录到二进制文件（binlog）中 从服务器（slave）把主服务器的二进制日志复制到自己的中继日志（relay log）中 从服务器重做中继日志中的日志，把更改应用到自己的数据库上，以达到数据的最终一致性。 快照+复制的备份架构复制可以用来备份，单功能不仅限于备份，其主要功能如下： 数据分布。由于MySQL数据库提供的复制并不需要很大的带宽要求，因此可以在不同的数据中心之间实现数据的复制。 读取的负载平衡。通过建立多个从服务器，可将读取平均地分配到这些从服务器中，并且减少主服务器的压力。一般通过DNS的Round-Robin和Linux的LVS功能都可以实现负载均衡 数据库备份。复制对备份很有帮助，但是从服务器不是备份，不能完全代替备份 高可用性和故障转移。通过复制建立的从服务器有助于故障转移，减少故障的停机时间和恢复时间。 一个比较好的方法是通过对从服务器商店额数据所在分区做快照，因此来避免误操作对复制造成影响。当发生主服务器上的误操作时，只需要将从服务器上的快照进行恢复，然后再根据二进制日志进行point-in-time的恢复即可。快照+复制的备份架构如图：","link":"/2020/05/04/mysql/ch09-%E5%A4%87%E4%BB%BD%E4%B8%8E%E6%81%A2%E5%A4%8D/"},{"title":"ch01-MYSQL体系结构和存储引擎","text":"基本概念什么是数据库物理操作系统文件或其他形式文件类型的集合。在Mysql中，数据库文件可以是frm、MYD、MYI、ibd结尾的文件。 什么是数据库实例Mysql数据库实例由后台线程以及一个共享内存区组成。共享内存可以被运行的后台线程所共享。数据库实例才是真正用于操作数据库文件的程序。 Mysql被设计为一个单进程多线程架构的数据库，也就是说，Mysql数据库实例在系统上表现就是一个进程。 Mysql的配置文件Mysql的配置文件存放位置很多，可以通过 mysql —help命令查看配置文件的位置。在没有配置文件的情况下，Mysql会按照编译时的默认参数设置来启动实例。当同一个参数在多个配置文件中都存在时，Mysql会以读取到的最后一个配置文件中的参数为准。 配置文件中的datadir参数指定了数据库所在的路径。 MYSQL体系结构Mysql体系结构如下图所示 主要由以下部分组成： 连接池组件 管理服务和工具组件 SQL接口组件 查询分析器组件 优化器组件 缓冲（Cache）组件 插件式存储引擎 物理文件 Mysql数据库区别于其他数据库最重要的一个特点就是其插件式的表存储引擎。存储引擎是基于表的，而不是数据库的。 Mysql存储引擎 InnoDB存储引擎。支持事务，设计目标是在线事务处理（OLTP）应用，特点是行锁设计、支持外键、并支持类似于Oracle的非锁定读，即默认读取操作不会产生锁。InnoDB将数据放在一个逻辑的表空间中，这个表空间就像一个黑盒一样由InnoDB存储引擎自身进行管理，可以将每个InnoDB管理的表单独放到一个独立的ibd文件中。InnoDB通过使用多版本并发控制（MVCC）来获得高并发行，并且实现了SQL标准的4中隔离级别，同时使用一种被称为next-key locking的策略来避免幻读（phantom）现象的产生。除此之外，InnoDB存储引擎还提供了插入缓冲（insert buffer）、二次写（double write）、自适应哈希索引（adaptive hash index）、预读（read ahead）等高性能和高可用的功能。对于表中数据的存储，InnoDB存储引擎采用了聚集（clustered）的方式，因此每张表的存储都是按主键的顺序进行存放。如果没有显式地在表定义时指定主键，InnoDB存储引擎会为每一行生成一个6字节的ROWID，并以此作为主键。 MyISAM存储引擎。MyISAM存储引擎不支持事务、表锁设计，支持全文索引，主要面向一些OLAP（在线分析处理）数据库应用，此外MyISAM的缓冲池之缓存（cache）索引文件，而不是缓冲数据文件。MyISAM由MYD和MYI组成，MYD用来存放数据文件，MYI用来存放索引文件。对于MyISAM存储引擎表，MySQL数据库只缓存其索引文件，数据库文件的缓存交由操作系统本身来完成，这与其他使用LRU算法缓存数据的大部分数据库大不相同。 NDB存储引擎。数据全部存放在内存中。NDB的连接操作（JOIN）是在MySQL数据库层完成的，而不是在存储引擎层完成的。这意味着，复杂的连表操作需要巨大的网络开销，因此查询速度很慢。 Memory存储引擎。将表数据存放在内存中，默认使用hash索引。 Archive存储引擎。只支持INSERT和SELECT操作。非常适合存储归档数据，如日志信息。 Maria存储引擎。支持缓存数据和索引文件，应用了行锁设计，提供了MVCC功能，支持事务和非事务安全的选项，以及更好的BLOB字符类型的处理能力。","link":"/2020/05/04/mysql/ch01-MYSQL%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84%E5%92%8C%E5%AD%98%E5%82%A8%E5%BC%95%E6%93%8E/"}],"tags":[{"name":"jvm","slug":"jvm","link":"/tags/jvm/"},{"name":"spring","slug":"spring","link":"/tags/spring/"},{"name":"spring security","slug":"spring-security","link":"/tags/spring-security/"},{"name":"database","slug":"database","link":"/tags/database/"},{"name":"innodb","slug":"innodb","link":"/tags/innodb/"},{"name":"leetcode","slug":"leetcode","link":"/tags/leetcode/"},{"name":"tree","slug":"tree","link":"/tags/tree/"},{"name":"dp","slug":"dp","link":"/tags/dp/"},{"name":"Data Structure","slug":"Data-Structure","link":"/tags/Data-Structure/"},{"name":"hashmap","slug":"hashmap","link":"/tags/hashmap/"},{"name":"java","slug":"java","link":"/tags/java/"},{"name":"mysql","slug":"mysql","link":"/tags/mysql/"}],"categories":[{"name":"java","slug":"java","link":"/categories/java/"},{"name":"mysql","slug":"mysql","link":"/categories/mysql/"},{"name":"algorithms","slug":"algorithms","link":"/categories/algorithms/"},{"name":"computer science","slug":"computer-science","link":"/categories/computer-science/"},{"name":"data structure","slug":"data-structure","link":"/categories/data-structure/"},{"name":"notes","slug":"notes","link":"/categories/notes/"}]}